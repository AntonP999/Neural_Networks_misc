{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data import Field, Dataset, BPTTIterator\n",
    "from torchtext.datasets import WikiText2\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math \n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача: используя библиотеку torchtext, сделать генератор данных для обучения сети, подобной рассмотренной в лекции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина словаря:  245\n",
      "[('<unk>', 0), ('<pad>', 1), ('<eos>', 2), (' ', 3), ('e', 4), ('t', 5), ('a', 6), ('n', 7), ('i', 8), ('o', 9), ('r', 10), ('s', 11), ('h', 12), ('d', 13), ('l', 14), ('u', 15), ('c', 16), ('m', 17), ('f', 18), ('g', 19), ('p', 20), ('w', 21), ('b', 22), ('y', 23), ('k', 24), (',', 25), ('.', 26), ('v', 27), ('<', 28), ('>', 29)]\n"
     ]
    }
   ],
   "source": [
    "# Создает токенайзер для разбиения текста на символы.\n",
    "tokenize = lambda x: re.findall(\".\", x)\n",
    "# Создает переменную класса torchtext.data.Field, хранящую информацию, необходимую для препроцессинга текста.\n",
    "TEXT = Field(sequential=True, tokenize=tokenize, eos_token=\"<eos>\", lower=True)\n",
    "# Разбиение датасета WikiText2 на множества для обучения, валидации, проверки.\n",
    "train, valid, test = WikiText2.splits(TEXT)\n",
    "# Построение словаря.\n",
    "TEXT.build_vocab(train, vectors=\"glove.6B.200d\")\n",
    "# Проверка словаря. Каждому символу присвоено числовое значение.\n",
    "print(\"Длина словаря: \", len(list(TEXT.vocab.stoi.items())))\n",
    "print(list(TEXT.vocab.stoi.items())[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация переменных\n",
    "batch_size = 128\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создает BPTTIterator для разбиения корпуса на последовательные батчи с таргетом, сдвинутым на 1\n",
    "train_iter, valid_iter, test_iter = BPTTIterator.splits((train, valid, test),\n",
    "                                                         batch_size=batch_size,\n",
    "                                                         bptt_len=sequence_length,    \n",
    "                                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация переменных\n",
    "eval_batch_size = 128\n",
    "grad_clip = 0.1\n",
    "lr = 4.\n",
    "best_val_loss = None\n",
    "log_interval = 100\n",
    "\n",
    "weight_matrix = TEXT.vocab.vectors\n",
    "ntokens = weight_matrix.shape[0]\n",
    "nfeatures = weight_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, lnorm=False):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp) # (длина словаря, количество признаков)        \n",
    "        self.lnorm = None\n",
    "        if lnorm:\n",
    "            self.lnorm = nn.LayerNorm(ninp)\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)  # (кол-во признаков, скрытых состояний, слоев)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken) # (переводит из признаков в токен словаря)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Инициализация весов Embedding и FC\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.drop(self.encoder(x)) # (Выход: длина последовательности, batch_size, кол-во признаков)\n",
    "        if self.lnorm is not None:\n",
    "            emb = self.lnorm(emb)\n",
    "        # (Размерность входа соответствует)\n",
    "        output, hidden = self.rnn(emb, hidden) \n",
    "        # (output: длина последовательности, batch_size, кол-во скрытых)\n",
    "        # (hidden: 2 * (Кол-во слоев, batch_size, кол-во скрытых). 1-й h_n, 2-й c_n)\n",
    "        output = self.drop(output)\n",
    "        # (Вход: N, кол-во признаков (скрытых))\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        # (Выход: N, размер словаря)\n",
    "        # Возвращает (длина последовательности, batch_size, длина словаря)\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        # Инициализация скрытого состояния\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
    "                    weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
    "        else:\n",
    "            return weight.new(self.nlayers, bsz, self.nhid).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Валидация.\n",
    "def evaluate(data_iter):\n",
    "    model.eval()\n",
    "    total_loss = 0    \n",
    "    # Инициализирует hidden. Не ислользуется?\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    for i, batch_data in enumerate(data_iter):\n",
    "        # torchtext.data.BPTTIterator возвращает данные в таком формате:\n",
    "        data, targets = batch_data.text, batch_data.target\n",
    "        # Получает результат.\n",
    "        output, hidden = model(data)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        # Накапливает ошибку.\n",
    "        total_loss += criterion(output_flat, targets.view(-1)).item()\n",
    "    # Возвращает среднюю ошибку.\n",
    "    return total_loss / len(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение.\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0    \n",
    "    for batch, batch_data in enumerate(train_iter):\n",
    "        data, targets = batch_data.text, batch_data.target        \n",
    "        # Обнуление градиента между батчами.\n",
    "        #model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        # Получение выхода модели.\n",
    "        output, hidden = model(data)        \n",
    "        # Расчет и обратное распространение ошибки.\n",
    "        loss = criterion(output.view(-1, ntokens), targets.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # Обновление весов.\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        #for p in model.parameters():\n",
    "        #    p.data.add_(-lr, p.grad.data)            \n",
    "        optimizer.step()\n",
    "\n",
    "        # Накопление ошибки\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Вывод средней ошибки по log_interval.\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_iter), lr, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание модели.\n",
    "# (self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5)\n",
    "model = RNNModel('LSTM', ntokens, 128, 128, 2, 0.3, lnorm=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1e+2, weight_decay=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эксперименты с оптимизаторами показали лучший результат после 1й эпохи для Adadelta с lr=100 (loss около 1.7). 2 слоя оказались лучше, чем 1 и 3 (тоже после 1й эпохи). GRU не удалось превзойти результат LSTM. Добавление LayerNormalization немного уменьшило ошибку, и сложилось впечатление, что после первой эпохи стали появляться более осмысленные слова. Возможно, случайность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация последовательности.\n",
    "def generate(n=50, temp=1.):\n",
    "    model.eval()\n",
    "    # Генерация начального символа.\n",
    "    x = torch.rand(1, 1).mul(ntokens).long()\n",
    "    hidden = None\n",
    "    out = []\n",
    "    # Создание последовательности длины n.\n",
    "    for i in range(n):\n",
    "        output, hidden = model(x, hidden)\n",
    "        # Получает распределение на следующую букву.\n",
    "        s_weights = output.squeeze().data.div(temp).exp()\n",
    "        # Сэмплирует индекс из распределения и принимает за следующий.\n",
    "        s_idx = torch.multinomial(s_weights, 1)[0]\n",
    "        x.data.fill_(s_idx)\n",
    "        # Переводит индекс в букву и дополняет последовательность.\n",
    "        s = TEXT.vocab.itos[s_idx]\n",
    "        out.append(s)\n",
    "    # Возвращает строку.\n",
    "    return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:\n",
      " 攻q–ṃoử–⁄αóóắァơcāấq0隊่჻5≤оşоμ°̃bตbʿ£е่隊¡₤aịầァ3'þdx機 \n",
      "\n",
      "| epoch   1 |   100/ 2808 batches | lr 4.00 | loss  2.97 | ppl    19.55\n",
      "| epoch   1 |   200/ 2808 batches | lr 4.00 | loss  2.19 | ppl     8.94\n",
      "| epoch   1 |   300/ 2808 batches | lr 4.00 | loss  2.07 | ppl     7.92\n",
      "| epoch   1 |   400/ 2808 batches | lr 4.00 | loss  2.00 | ppl     7.41\n",
      "| epoch   1 |   500/ 2808 batches | lr 4.00 | loss  1.95 | ppl     7.04\n",
      "| epoch   1 |   600/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.84\n",
      "| epoch   1 |   700/ 2808 batches | lr 4.00 | loss  1.90 | ppl     6.68\n",
      "| epoch   1 |   800/ 2808 batches | lr 4.00 | loss  1.88 | ppl     6.55\n",
      "| epoch   1 |   900/ 2808 batches | lr 4.00 | loss  1.86 | ppl     6.46\n",
      "| epoch   1 |  1000/ 2808 batches | lr 4.00 | loss  1.85 | ppl     6.37\n",
      "| epoch   1 |  1100/ 2808 batches | lr 4.00 | loss  1.84 | ppl     6.30\n",
      "| epoch   1 |  1200/ 2808 batches | lr 4.00 | loss  1.83 | ppl     6.25\n",
      "| epoch   1 |  1300/ 2808 batches | lr 4.00 | loss  1.82 | ppl     6.17\n",
      "| epoch   1 |  1400/ 2808 batches | lr 4.00 | loss  1.79 | ppl     6.02\n",
      "| epoch   1 |  1500/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.04\n",
      "| epoch   1 |  1600/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.05\n",
      "| epoch   1 |  1700/ 2808 batches | lr 4.00 | loss  1.79 | ppl     6.01\n",
      "| epoch   1 |  1800/ 2808 batches | lr 4.00 | loss  1.79 | ppl     6.00\n",
      "| epoch   1 |  1900/ 2808 batches | lr 4.00 | loss  1.79 | ppl     6.01\n",
      "| epoch   1 |  2000/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.89\n",
      "| epoch   1 |  2100/ 2808 batches | lr 4.00 | loss  1.78 | ppl     5.96\n",
      "| epoch   1 |  2200/ 2808 batches | lr 4.00 | loss  1.78 | ppl     5.94\n",
      "| epoch   1 |  2300/ 2808 batches | lr 4.00 | loss  1.78 | ppl     5.94\n",
      "| epoch   1 |  2400/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.86\n",
      "| epoch   1 |  2500/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.82\n",
      "| epoch   1 |  2600/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.87\n",
      "| epoch   1 |  2700/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.81\n",
      "| epoch   1 |  2800/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss  1.55 | valid ppl     4.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  to back that storm ( 5 ken variousling of the <un \n",
      "\n",
      "Epoch time: 14.979926311969757\n",
      "| epoch   2 |   100/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.87\n",
      "| epoch   2 |   200/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.73\n",
      "| epoch   2 |   300/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.73\n",
      "| epoch   2 |   400/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.74\n",
      "| epoch   2 |   500/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.69\n",
      "| epoch   2 |   600/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.72\n",
      "| epoch   2 |   700/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.68\n",
      "| epoch   2 |   800/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.71\n",
      "| epoch   2 |   900/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.68\n",
      "| epoch   2 |  1000/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.66\n",
      "| epoch   2 |  1100/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.65\n",
      "| epoch   2 |  1200/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.66\n",
      "| epoch   2 |  1300/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.65\n",
      "| epoch   2 |  1400/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   2 |  1500/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
      "| epoch   2 |  1600/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.63\n",
      "| epoch   2 |  1700/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.61\n",
      "| epoch   2 |  1800/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.63\n",
      "| epoch   2 |  1900/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.69\n",
      "| epoch   2 |  2000/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
      "| epoch   2 |  2100/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.64\n",
      "| epoch   2 |  2200/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.66\n",
      "| epoch   2 |  2300/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.65\n",
      "| epoch   2 |  2400/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
      "| epoch   2 |  2500/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   2 |  2600/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.66\n",
      "| epoch   2 |  2700/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "| epoch   2 |  2800/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss  1.51 | valid ppl     4.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " 1963 , like he delief ) ( an in they hallel broad  \n",
      "\n",
      "Epoch time: 14.953179597854614\n",
      "| epoch   3 |   100/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.68\n",
      "| epoch   3 |   200/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.53\n",
      "| epoch   3 |   300/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch   3 |   400/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   3 |   500/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   3 |   600/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   3 |   700/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   3 |   800/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   3 |   900/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   3 |  1000/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.53\n",
      "| epoch   3 |  1100/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.50\n",
      "| epoch   3 |  1200/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.52\n",
      "| epoch   3 |  1300/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.53\n",
      "| epoch   3 |  1400/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.44\n",
      "| epoch   3 |  1500/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.48\n",
      "| epoch   3 |  1600/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.53\n",
      "| epoch   3 |  1700/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.50\n",
      "| epoch   3 |  1800/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.52\n",
      "| epoch   3 |  1900/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "| epoch   3 |  2000/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.50\n",
      "| epoch   3 |  2100/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.52\n",
      "| epoch   3 |  2200/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch   3 |  2300/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
      "| epoch   3 |  2400/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.48\n",
      "| epoch   3 |  2500/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.49\n",
      "| epoch   3 |  2600/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch   3 |  2700/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.50\n",
      "| epoch   3 |  2800/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss  1.47 | valid ppl     4.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " e history ( ut to united season world was the 77 @ \n",
      "\n",
      "Epoch time: 14.985126320521037\n",
      "| epoch   4 |   100/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   4 |   200/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.45\n",
      "| epoch   4 |   300/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.48\n",
      "| epoch   4 |   400/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.48\n",
      "| epoch   4 |   500/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.46\n",
      "| epoch   4 |   600/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.45\n",
      "| epoch   4 |   700/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.44\n",
      "| epoch   4 |   800/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.46\n",
      "| epoch   4 |   900/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.46\n",
      "| epoch   4 |  1000/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.48\n",
      "| epoch   4 |  1100/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.44\n",
      "| epoch   4 |  1200/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.47\n",
      "| epoch   4 |  1300/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.45\n",
      "| epoch   4 |  1400/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.36\n",
      "| epoch   4 |  1500/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  1600/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.46\n",
      "| epoch   4 |  1700/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.45\n",
      "| epoch   4 |  1800/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.45\n",
      "| epoch   4 |  1900/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.53\n",
      "| epoch   4 |  2000/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.42\n",
      "| epoch   4 |  2100/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.48\n",
      "| epoch   4 |  2200/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.50\n",
      "| epoch   4 |  2300/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.51\n",
      "| epoch   4 |  2400/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.45\n",
      "| epoch   4 |  2500/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.44\n",
      "| epoch   4 |  2600/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.53\n",
      "| epoch   4 |  2700/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.47\n",
      "| epoch   4 |  2800/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss  1.49 | valid ppl     4.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  <unk> and the buelf of east of an newsshrip that  \n",
      "\n",
      "Epoch time: 15.014783040682476\n",
      "| epoch   5 |   100/ 2808 batches | lr 1.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   5 |   200/ 2808 batches | lr 1.00 | loss  1.69 | ppl     5.41\n",
      "| epoch   5 |   300/ 2808 batches | lr 1.00 | loss  1.69 | ppl     5.44\n",
      "| epoch   5 |   400/ 2808 batches | lr 1.00 | loss  1.70 | ppl     5.45\n",
      "| epoch   5 |   500/ 2808 batches | lr 1.00 | loss  1.70 | ppl     5.46\n",
      "| epoch   5 |   600/ 2808 batches | lr 1.00 | loss  1.69 | ppl     5.42\n",
      "| epoch   5 |   700/ 2808 batches | lr 1.00 | loss  1.69 | ppl     5.42\n",
      "| epoch   5 |   800/ 2808 batches | lr 1.00 | loss  1.69 | ppl     5.44\n",
      "| epoch   5 |   900/ 2808 batches | lr 1.00 | loss  1.69 | ppl     5.43\n",
      "| epoch   5 |  1000/ 2808 batches | lr 1.00 | loss  1.69 | ppl     5.43\n",
      "| epoch   5 |  1100/ 2808 batches | lr 1.00 | loss  1.69 | ppl     5.41\n",
      "| epoch   5 |  1200/ 2808 batches | lr 1.00 | loss  1.69 | ppl     5.43\n",
      "| epoch   5 |  1300/ 2808 batches | lr 1.00 | loss  1.69 | ppl     5.41\n",
      "| epoch   5 |  1400/ 2808 batches | lr 1.00 | loss  1.68 | ppl     5.36\n",
      "| epoch   5 |  1500/ 2808 batches | lr 1.00 | loss  1.68 | ppl     5.35\n",
      "| epoch   5 |  1600/ 2808 batches | lr 1.00 | loss  1.69 | ppl     5.41\n",
      "| epoch   5 |  1700/ 2808 batches | lr 1.00 | loss  1.68 | ppl     5.37\n",
      "| epoch   5 |  1800/ 2808 batches | lr 1.00 | loss  1.69 | ppl     5.40\n",
      "| epoch   5 |  1900/ 2808 batches | lr 1.00 | loss  1.70 | ppl     5.47\n",
      "| epoch   5 |  2000/ 2808 batches | lr 1.00 | loss  1.69 | ppl     5.40\n",
      "| epoch   5 |  2100/ 2808 batches | lr 1.00 | loss  1.70 | ppl     5.45\n",
      "| epoch   5 |  2200/ 2808 batches | lr 1.00 | loss  1.69 | ppl     5.44\n",
      "| epoch   5 |  2300/ 2808 batches | lr 1.00 | loss  1.70 | ppl     5.47\n",
      "| epoch   5 |  2400/ 2808 batches | lr 1.00 | loss  1.68 | ppl     5.38\n",
      "| epoch   5 |  2500/ 2808 batches | lr 1.00 | loss  1.69 | ppl     5.39\n",
      "| epoch   5 |  2600/ 2808 batches | lr 1.00 | loss  1.70 | ppl     5.47\n",
      "| epoch   5 |  2700/ 2808 batches | lr 1.00 | loss  1.69 | ppl     5.41\n",
      "| epoch   5 |  2800/ 2808 batches | lr 1.00 | loss  1.68 | ppl     5.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss  1.49 | valid ppl     4.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " as ' foot .b30s . an all of forsing streak of the  \n",
      "\n",
      "Epoch time: 15.094863188266753\n"
     ]
    }
   ],
   "source": [
    "# Обучение, валидация и вывод результатов.\n",
    "with torch.no_grad():\n",
    "    print('sample:\\n', generate(50), '\\n')\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    start_time = time()\n",
    "    train()\n",
    "    val_loss = evaluate(valid_iter)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "        epoch, val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "    with torch.no_grad():\n",
    "        print('sample:\\n', generate(50), '\\n')\n",
    "    print(\"Epoch time: {}\".format((time()-start_time)/60.))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После 5 эпох ошибка так и не смогла дальше снизится. Конечно, нужно было подбирать параметры на более, чем одной эпохе, но очень уж медленный pytorch. Осталось пространство для экспериментов. Интересно еще попробовать последовательность слов."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
