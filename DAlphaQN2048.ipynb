{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "ikumCeWbkRlj",
    "outputId": "6ddaa3e2-2e6e-45cc-e9b4-ea5c43d31709"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "6hh61vOuzzpD",
    "outputId": "17d74b3f-67ba-448e-b527-c32d09008b5a"
   },
   "outputs": [],
   "source": [
    "PATH = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s0uMULEQkWIN"
   },
   "outputs": [],
   "source": [
    "param_join_reward = 1\n",
    "param_inv_move_reward = -8\n",
    "param_ohe_state = True\n",
    "param_input_shape = 4*4*14 if param_ohe_state else 4*4\n",
    "param_n_actions = 4\n",
    "#param_loss_fn = F.mse_loss\n",
    "#param_gamma = 0.99\n",
    "#param_epsilon = 0.1\n",
    "#param_iterations = 1000\n",
    "param_batch_size = 64\n",
    "#param_replay_memory_size = param_batch_size*4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8XWuHir3kY6i"
   },
   "outputs": [],
   "source": [
    "# Направление хода -> оси\n",
    "directions_dict = {0: np.array((0,1)),\n",
    "                   1: np.array((1,0)),\n",
    "                   2: np.array((0,-1)),\n",
    "                   3: np.array((-1,0)),}\n",
    "\n",
    "# Направление в one-hot\n",
    "def dir_to_ohe(direction_int):\n",
    "    ohe = np.zeros(4)\n",
    "    ohe[direction_int] = 1\n",
    "    return ohe\n",
    "\n",
    "# Направление one-hot в int\n",
    "def ohe_to_dir(direction_ohe):\n",
    "    return np.argmax(direction_ohe)\n",
    "\n",
    "# Значение тайла в one-hot\n",
    "def tile_to_ohe(n):    \n",
    "    ohe = np.zeros(14)\n",
    "    if n>0:\n",
    "        n = int(np.log2(n))    \n",
    "        ohe[n-1] = 1\n",
    "    return ohe\n",
    "\n",
    "# Состояние в one-hot для нейросети\n",
    "def state_to_ohe(state):\n",
    "    ohe = np.zeros((4,4,14))\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            ohe[i,j] = tile_to_ohe(state[i,j])\n",
    "    return ohe.transpose((2,0,1))\n",
    "\n",
    "def position_to_tuple(position):\n",
    "    return tuple([tuple(x) for x in position])\n",
    "\n",
    "# Игровой движок\n",
    "class Game_Core_2048:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, random_start=0., max_number=128):\n",
    "        # Стандартное начало игры или со случайного состояния\n",
    "        self.gameboard = np.zeros((4, 4)).astype(int)\n",
    "        if np.random.rand() >= random_start:\n",
    "            self.place_random_number()\n",
    "            self.place_random_number()\n",
    "        else:\n",
    "            self.make_random_state(max_number)\n",
    "        #self.gameover = False\n",
    "        self.moves_dict = self.find_all_moves()\n",
    "        self.gameover = self.check_gameover(self.moves_dict)\n",
    "        self.score = 0\n",
    "        self.inv_move_count = 0\n",
    "\n",
    "    def random_free_cell(self):\n",
    "        # Возвращает случайную пустую ячейку\n",
    "        free_cells = np.argwhere(self.gameboard == 0)\n",
    "        return free_cells[np.random.randint(len(free_cells))]\n",
    "\n",
    "    def place_random_number(self):\n",
    "        # Добавление случайного тайла (по принципу как в оригинале)\n",
    "        n = 2 if np.random.rand() < 0.9 else 4\n",
    "        cell = self.random_free_cell()\n",
    "        self.gameboard[cell[0], cell[1]] = n\n",
    "\n",
    "    def place_number(self, position, state):\n",
    "        # Ставит значение в позицию (для MCTS). \n",
    "        state = state.copy()\n",
    "        free_cells = np.argwhere(state == 0)\n",
    "        number = 2\n",
    "        if position >= len(free_cells):\n",
    "            number = 4\n",
    "            position -= len(free_cells)\n",
    "        cell = free_cells[position]\n",
    "        state[cell[0], cell[1]] = number\n",
    "        moves_dict = self.find_all_moves(state)\n",
    "        state_gameover = self.check_gameover(moves_dict)\n",
    "        return state, state_gameover\n",
    "\n",
    "    def make_random_state(self, max_number):\n",
    "        # Генерирует случайную стартовую позицию.\n",
    "        idx = np.where(self.gameboard==0)\n",
    "        powers = np.random.randint(np.log2(max_number), size=(len(idx[0])))+1\n",
    "        mask = np.random.randint(2, size=(len(idx[0]))).astype(np.bool)\n",
    "        self.gameboard[idx] = 2**powers*mask\n",
    "\n",
    "    def find_moves(self, gameboard, direction, already_summed_mask, offset):\n",
    "        # Возвращает допустимые ходы в заданном направлении для заданного ряда/колонки.\n",
    "        # Пытался избежать вложенных циклов, как в оригинальной версии.\n",
    "        keep0 = []  # Хранение ходов - ось 0\n",
    "        keep1 = []  # Хранение ходов - ось 1\n",
    "        axis = np.argwhere(direction != 0)[0, 0]  # Ось направления\n",
    "        del_axis = int(not axis)  # Ось удаления найденных ходов\n",
    "\n",
    "        # Выбор параметров для горизонтального или вертикального направления.\n",
    "        if axis:\n",
    "            shape = (4, 1)\n",
    "            inds = np.indices(shape)\n",
    "            dirs = np.tile(direction[axis], 4).reshape(shape)\n",
    "            mask = np.ones(shape) # Маска для несдвигаемых тайлов.\n",
    "        else:\n",
    "            shape = (1, 4)\n",
    "            inds = np.indices(shape)\n",
    "            dirs = np.tile(direction[axis], 4).reshape(shape)\n",
    "            mask = np.ones(shape) # Маска для несдвигаемых тайлов.\n",
    "\n",
    "        #move_inds = inds.copy()\n",
    "        check_inds = inds.copy()  # Индексы для проверки доступного хода вдоль оси\n",
    "        check_inds[axis] = check_inds[axis] + offset  # Начало со смещения\n",
    "        mask = (mask * (gameboard[check_inds[0], check_inds[1]] != 0)).astype(bool)  # Нули не двигаем\n",
    "        keep0, keep1 = check_inds[0][mask == 0], check_inds[1][mask == 0]  # Сохранение перед удалением\n",
    "        if axis:\n",
    "            cur_values = gameboard[:, offset].reshape(shape)  # Сохранение текущих значений в ячейках\n",
    "        else:\n",
    "            cur_values = gameboard[offset, :].reshape(shape)  # Сохранение текущих значений в ячейках\n",
    "        cur_values_mask = np.ones(shape).astype(bool)\n",
    "        # Удаление из поиска того, что дальше не двигается.\n",
    "        check_inds = np.delete(check_inds, np.where(mask.flatten() == 0), axis=del_axis + 1)\n",
    "        dirs = np.delete(dirs, np.where(mask.flatten() == 0), axis=del_axis)\n",
    "        cur_values = np.delete(cur_values, np.where(mask.flatten() == 0), axis=del_axis)\n",
    "        cur_values_mask = np.delete(cur_values_mask, np.where(mask.flatten() == 0), axis=del_axis)\n",
    "        mask = np.delete(mask, np.where(mask.flatten() == 0), axis=del_axis)\n",
    "\n",
    "        # Пока что-то можно двигать дальше\n",
    "        while check_inds.size > 0:\n",
    "            cur_values[cur_values_mask] = gameboard[check_inds[0], check_inds[1]][cur_values_mask] # Сохранение текущих значений в ячейках\n",
    "            prev_inds = check_inds.copy()\n",
    "            check_inds[axis] = check_inds[axis] + dirs * mask # Смещение индексов для проверки на 1 по направлению.\n",
    "            mask = ((-direction[axis] * check_inds[axis]) >= 1 * (direction[axis] > 0)).astype(\n",
    "                bool)  # Проверка на границы поля\n",
    "            asm_mask = already_summed_mask[check_inds[0], check_inds[1]] == 0  # Проверка на уже суммированное значение (всегда первое по направлению)\n",
    "            mask = mask * asm_mask\n",
    "            new_values = gameboard[check_inds[0], check_inds[1]]  # Проверка на равенство\n",
    "            cur_values_mask = new_values != 0\n",
    "            mask_eq = mask * (cur_values == new_values)\n",
    "            mask_free = mask * (gameboard[check_inds[0], check_inds[1]] == 0)  # Проверка на свободную ячейку\n",
    "            mask = mask_eq + mask_free\n",
    "            keep0 = np.r_[keep0, prev_inds[0][mask == 0]] # Сохранение завершенных сдвигов перед удалением из поиска\n",
    "            keep1 = np.r_[keep1, prev_inds[1][mask == 0]]\n",
    "            # Удаление из поиска того, что дальше не двигается.\n",
    "            check_inds = np.delete(check_inds, np.where(mask.flatten() == 0), axis=del_axis + 1)\n",
    "            dirs = np.delete(dirs, np.where(mask.flatten() == 0), axis=del_axis)\n",
    "            cur_values = np.delete(cur_values, np.where(mask.flatten() == 0), axis=del_axis)\n",
    "            cur_values_mask = np.delete(cur_values_mask, np.where(mask.flatten() == 0), axis=del_axis)\n",
    "            mask = np.delete(mask, np.where(mask.flatten() == 0), axis=del_axis)\n",
    "\n",
    "        # Создание возвращаемых массивов.\n",
    "        moves = np.c_[keep0.reshape(-1, 1), keep1.reshape(-1, 1)]\n",
    "        if axis:\n",
    "            moves = moves[moves[:, 0].argsort(axis=0)]\n",
    "            mask = moves[:, 1] != offset\n",
    "        else:\n",
    "            moves = moves[moves[:, 1].argsort(axis=0)]\n",
    "            mask = moves[:, 0] != offset\n",
    "\n",
    "        return moves, mask\n",
    "\n",
    "    def get_offset(self, offset, direction):\n",
    "        # Возвращает смещение для направления\n",
    "        axis = np.argwhere(direction != 0)[0, 0]\n",
    "        offset = -direction[axis] * offset\n",
    "        if direction[axis] > 0:\n",
    "            offset = offset - 1\n",
    "        return offset, axis\n",
    "\n",
    "    def find_all_moves(self, state=None):\n",
    "        # Поиск всех возможных ходов для состояния.\n",
    "        moves_dict = {}\n",
    "        # Проверка направлений\n",
    "        for d in range(4):\n",
    "            direction = directions_dict[d]\n",
    "            if state is None:\n",
    "                temp_gb = self.gameboard.copy()\n",
    "            else:\n",
    "                temp_gb = state.copy()\n",
    "            already_summed_mask = np.zeros_like(temp_gb)\n",
    "            dir_dict = {}\n",
    "            moves_available = False\n",
    "            # Проверка смещений\n",
    "            for offset in range(3):\n",
    "                offset, axis = self.get_offset(offset + 1, direction)\n",
    "                moves, mask = self.find_moves(temp_gb, direction, already_summed_mask, offset)\n",
    "                dir_dict[offset] = (moves, mask)\n",
    "                if not moves_available:\n",
    "                    moves_available = np.any(mask)\n",
    "                already_summed_mask[moves[mask, 0], moves[mask, 1]] = temp_gb[moves[mask, 0], moves[mask, 1]] != 0\n",
    "                if axis:\n",
    "                    temp_gb[moves[mask, 0], moves[mask, 1]] = temp_gb[moves[mask, 0], moves[mask, 1]] + temp_gb[\n",
    "                        mask, offset]\n",
    "                    temp_gb[mask, offset] = 0\n",
    "                else:\n",
    "                    temp_gb[moves[mask, 0], moves[mask, 1]] = temp_gb[moves[mask, 0], moves[mask, 1]] + temp_gb[\n",
    "                        offset, mask]\n",
    "                    temp_gb[offset, mask] = 0\n",
    "            moves_dict[d] = (moves_available, dir_dict)\n",
    "        # Возвращает словарь {Направление : (допустимость хода в направлении, сдвигаемые ячейки)}\n",
    "        return moves_dict\n",
    "\n",
    "    def check_gameover(self, moves_dict):\n",
    "        # Проверка на конец игры\n",
    "        return not np.any([vm[0] for vm in moves_dict.values()])\n",
    "\n",
    "    def move(self, direction):\n",
    "        # Движение в заданном направлении.\n",
    "        if not self.gameover:\n",
    "            movescore = 0\n",
    "            reward = 0\n",
    "            terminal = False\n",
    "            if self.moves_dict[direction][0]:\n",
    "                # Если направление валидно\n",
    "                self.inv_move_count = 0\n",
    "                invalid_move = False\n",
    "                self.undo = self.gameboard.copy()\n",
    "                all_moves = self.moves_dict[direction][1]\n",
    "                axis = np.argwhere(directions_dict[direction] != 0)[0, 0]\n",
    "                for offset, (moves, mask) in all_moves.items():\n",
    "                    # Перемещение тайлов для каждого из трех смещений\n",
    "                    if axis:                        \n",
    "                        score_delta = self.gameboard[moves[mask, 0], moves[mask, 1]].sum()*2\n",
    "                        movescore += score_delta  # Score\n",
    "                        #if score_delta:\n",
    "                            #reward += param_join_reward * (self.gameboard[moves[mask, 0], moves[mask, 1]]!=0).sum()  \n",
    "                        # Сдвиг или суммирование на новой позиции                      \n",
    "                        self.gameboard[moves[mask, 0], moves[mask, 1]] = self.gameboard[\n",
    "                                                                             moves[mask, 0], moves[mask, 1]] + \\\n",
    "                                                                         self.gameboard[mask, offset]\n",
    "                        # Обнуление старой позиции\n",
    "                        self.gameboard[mask, offset] = 0\n",
    "                    else:                        \n",
    "                        score_delta = self.gameboard[moves[mask, 0], moves[mask, 1]].sum()*2\n",
    "                        movescore += score_delta\n",
    "                        #if score_delta:\n",
    "                            #reward += param_join_reward * (self.gameboard[moves[mask, 0], moves[mask, 1]]!=0).sum()\n",
    "                        self.gameboard[moves[mask, 0], moves[mask, 1]] = self.gameboard[\n",
    "                                                                             moves[mask, 0], moves[mask, 1]] + \\\n",
    "                                                                         self.gameboard[offset, mask]\n",
    "                        self.gameboard[offset, mask] = 0\n",
    "                # Подсчет очков\n",
    "                self.score += movescore\n",
    "                reward = movescore                \n",
    "                # Добавление случайного тайла\n",
    "                self.place_random_number()\n",
    "                # Проверка на возможность продолжения игры\n",
    "                self.moves_dict = self.find_all_moves()\n",
    "                self.gameover = self.check_gameover(self.moves_dict)\n",
    "                if self.gameover:\n",
    "                    terminal = True                    \n",
    "            else:\n",
    "                # Ход был недопустимым. Подсчет количества для invalid_move_tolerance                \n",
    "                self.inv_move_count += 1\n",
    "                invalid_move = True\n",
    "                reward = param_inv_move_reward\n",
    "            \n",
    "            return reward, terminal, invalid_move\n",
    "        else:\n",
    "            # Игра уже закончена            \n",
    "            return 0, True, False\n",
    "\n",
    "    def restore(self):\n",
    "        # Undo\n",
    "        self.gameboard = self.undo.copy()\n",
    "        self.moves_dict = self.find_all_moves()\n",
    "\n",
    "    def move_from_state(self, state, direction, moves_dict=None):\n",
    "        # Движение в заданном направлении из искусственного стейта для MCTS. (Копипэйст move(self) из-за недостатка времени)\n",
    "        state = state.copy()\n",
    "        if moves_dict is None:\n",
    "            moves_dict = self.find_all_moves(state)\n",
    "        state_gameover = self.check_gameover(moves_dict)        \n",
    "        if not state_gameover:\n",
    "            movescore = 0\n",
    "            reward = 0\n",
    "            terminal = False\n",
    "            if moves_dict[direction][0]:\n",
    "                inv_move_count = 0\n",
    "                invalid_move = False                \n",
    "                all_moves = moves_dict[direction][1]\n",
    "                axis = np.argwhere(directions_dict[direction] != 0)[0, 0]\n",
    "                for offset, (moves, mask) in all_moves.items():                    \n",
    "                    if axis:                        \n",
    "                        score_delta = state[moves[mask, 0], moves[mask, 1]].sum()*2\n",
    "                        movescore += score_delta  # Score\n",
    "                        #if score_delta:\n",
    "                            #reward += param_join_reward * (self.gameboard[moves[mask, 0], moves[mask, 1]]!=0).sum()                        \n",
    "                        state[moves[mask, 0], moves[mask, 1]] = state[moves[mask, 0], moves[mask, 1]] + \\\n",
    "                                                                         state[mask, offset]\n",
    "                        state[mask, offset] = 0\n",
    "                    else:                        \n",
    "                        score_delta = state[moves[mask, 0], moves[mask, 1]].sum()*2\n",
    "                        movescore += score_delta\n",
    "                        #if score_delta:\n",
    "                            #reward += param_join_reward * (self.gameboard[moves[mask, 0], moves[mask, 1]]!=0).sum()\n",
    "                        state[moves[mask, 0], moves[mask, 1]] = state[moves[mask, 0], moves[mask, 1]] + \\\n",
    "                                                                         state[offset, mask]\n",
    "                        state[offset, mask] = 0\n",
    "                #self.score += movescore\n",
    "                reward = movescore                \n",
    "                #self.place_random_number()\n",
    "                moves_dict = self.find_all_moves(state)\n",
    "                state_gameover = self.check_gameover(moves_dict)\n",
    "                if state_gameover:\n",
    "                    terminal = True                    \n",
    "            else:                \n",
    "                #inv_move_count += 1\n",
    "                invalid_move = True\n",
    "                reward = 0 #param_inv_move_reward\n",
    "            \n",
    "            return reward, state, terminal, invalid_move\n",
    "        else:            \n",
    "            return 0, True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UvnqHp8Skeni"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Environment. Изначально создавался под DQN (action -> reward, state, terminal) и GUI от pygame.\n",
    "Пришлось вносить срочные изменения при переходе на метод AlphaGo.\n",
    "Сейчас частично используются методы отсюда, частично из GameCore (в MCTS).\n",
    "ToDo : Привести в нормальный вид.\n",
    "\"\"\"\n",
    "class Env2048():\n",
    "    def __init__(self, gui=True, inv_move_tolerance=0):\n",
    "        self.game_core = Game_Core_2048()\n",
    "        #self.player = player\n",
    "        self.gui = None\n",
    "        if gui:\n",
    "            self.gui = Game2048(self.game_core)\n",
    "        self.inv_move_tolerance = inv_move_tolerance\n",
    "        self.inv_move_count = 0\n",
    "        #if self.player:\n",
    "         #   self.player.start()\n",
    "\n",
    "#    def draw_game(self):\n",
    "#        self.game.surface.fill(colors.AZURE3)\n",
    "\n",
    "        #self.handle_events()\n",
    "#        self.game.update()\n",
    "#        self.game.draw()\n",
    "#        pygame.display.update()\n",
    "#        self.game.clock.tick(self.game.frame_rate)\n",
    "\n",
    "    def reset(self, random_start=0., max_number=128):\n",
    "        self.game_core.reset(random_start, max_number)\n",
    "        self.inv_move_count = 0\n",
    "\n",
    "    def get_state(self):\n",
    "        state = self.game_core.gameboard.copy()\n",
    "        #if ohe:\n",
    "         #   state = state_to_ohe(state).transpose((2,0,1))        \n",
    "        return state\n",
    "\n",
    "    def act(self, action_ohe, ohe_state):\n",
    "        direction = ohe_to_dir(action_ohe)\n",
    "        state = self.get_state()\n",
    "        reward, terminal, invalid_move = self.game_core.move(direction)\n",
    "        new_state = self.get_state()\n",
    "        if not invalid_move:\n",
    "            self.inv_move_count = 0\n",
    "            if self.gui:\n",
    "                self.gui.move()\n",
    "        else:\n",
    "            self.inv_move_count += 1            \n",
    "            if self.inv_move_tolerance and self.inv_move_count >= self.inv_move_tolerance:\n",
    "                terminal = True\n",
    "        return state, action_ohe, reward, new_state, terminal\n",
    "\n",
    "    def act_from_state(self, state, direction):\n",
    "        #direction = ohe_to_dir(action_ohe)\n",
    "        #state = self.get_state(ohe_state)\n",
    "        reward, new_state, terminal, invalid_move = self.game_core.move_from_state(direction)\n",
    "        #new_state = self.get_state(ohe_state)\n",
    "        if invalid_move:\n",
    "            terminal = True            \n",
    "        return reward, new_state, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oMCcUzM76rQP"
   },
   "outputs": [],
   "source": [
    "def softmax(Q):\n",
    "    return np.exp(Q)/np.exp(Q).sum()\n",
    "\n",
    "# Node для MCTS\n",
    "class TreeNode():\n",
    "    def __init__(self, game_core, state, reward, player, net, parent=None, parent_a=-1, terminal=False):\n",
    "        self.game_core = game_core\n",
    "        self.player = player\n",
    "        self.state = state\n",
    "        self.score = 0\n",
    "        self.reward = reward\n",
    "        self.parent = parent\n",
    "        self.parent_a = parent_a\n",
    "        self.moves_dict = None\n",
    "        self.possible_moves = 1\n",
    "        self.terminal = terminal   \n",
    "        self.v = 0     \n",
    "        \n",
    "        \n",
    "\n",
    "        if self.player:\n",
    "            # Получение Q_pred  от сети\n",
    "            with torch.no_grad():\n",
    "                self.Q_pred = net(torch.Tensor(state_to_ohe(state)).unsqueeze(0).to(device))\n",
    "                self.Q_pred = self.Q_pred.detach().cpu().numpy().reshape(-1)\n",
    "                self.P = softmax(self.Q_pred)                \n",
    "            if parent is None:\n",
    "                self.P = self.P*0.75 + np.random.dirichlet((0.03,0.03,0.03,0.03))*0.25\n",
    "            self.moves_dict = self.game_core.find_all_moves(self.state)\n",
    "            self.possible_moves = np.array(list(zip(*self.moves_dict.values()))[0]).astype(int)\n",
    "            #self.P *= self.possible_moves\n",
    "            self.N = np.zeros(4)\n",
    "            self.Q_star = np.zeros(4)\n",
    "            self.Q_totals = np.zeros(4)\n",
    "        else:\n",
    "            # Распределение вероятностей появления случайных тайлов для p, если ход не игрока. \n",
    "            freecells = np.argwhere(self.state == 0)            \n",
    "            self.P = np.array([0.9]*len(freecells) + [0.1]*len(freecells)) / len(freecells)            \n",
    "            self.N = np.zeros(len(freecells)*2)\n",
    "            self.Q_star = np.zeros(len(freecells)*2)\n",
    "            self.Q_totals = np.zeros(len(freecells)*2)\n",
    "        #self.V = 0.\n",
    "        self.c_puct = 1\n",
    "        self.number_of_visits = 0        \n",
    "        self.children = {}\n",
    "        self.has_children = False        \n",
    "\n",
    "    def get_Q_star(self, tau=1.):\n",
    "        # Возвращает Q_star\n",
    "        #pi = self.N**(1./tau) / (self.N**(1./tau)).sum()\n",
    "        return self.Q_star, self.possible_moves\n",
    "\n",
    "    def find_node(self, action, state):\n",
    "        # Возвращает нод с состоянием\n",
    "        if not self.player:\n",
    "            return None\n",
    "        if self.has_children:\n",
    "            if action in self.children.keys():                \n",
    "                node = self.children[action]\n",
    "                if node.has_children:\n",
    "                    for k, v in node.children.items():\n",
    "                        if np.all(state == v.state):\n",
    "                            return v                \n",
    "        return None\n",
    "\n",
    "    def make_root(self):        \n",
    "        # Делает нод корнем\n",
    "        self.movescore = 0\n",
    "        self.parent = None\n",
    "        self.parent_a = -1\n",
    "        self.P = self.P*0.75 + np.random.dirichlet((0.03,0.03,0.03,0.03))*0.25\n",
    "\n",
    "    def select_child(self):\n",
    "        # Выбирает куда идти по UCB\n",
    "        if self.terminal:            \n",
    "            return self, True\n",
    "\n",
    "        if self.player:            \n",
    "            #Qs = self.Q.sum() if self.Q.sum() != 0 else 1\n",
    "            U = (softmax(self.Q_star) + self.c_puct*self.P*(np.sqrt(self.N.sum())/(1+self.N)))*self.possible_moves\n",
    "            action = np.argmax(U)  \n",
    "            #print(\"Action: \", action)                \n",
    "        else:\n",
    "            action = np.random.choice(np.arange(len(self.P)), p = self.P)\n",
    "        \n",
    "        self.N[action] += 1\n",
    "        if self.has_children:\n",
    "            if action in self.children.keys():                \n",
    "                return self.children[action], False\n",
    "            else:                  \n",
    "                return self.expand(action), not self.player\n",
    "        else:            \n",
    "            return self.expand(action), not self.player\n",
    "\n",
    "    def expand(self, action):\n",
    "        # Создание нового нода\n",
    "        if self.player:\n",
    "            reward, new_state, terminal, invalid_move = self.game_core.move_from_state(self.state, action, self.moves_dict)\n",
    "            if invalid_move:\n",
    "                terminal = True            \n",
    "            self.children[action] = TreeNode(self.game_core, new_state, np.log1p(reward), not self.player, net, self, action, terminal)\n",
    "        else:\n",
    "            new_state, terminal = self.game_core.place_number(action, self.state)            \n",
    "            self.children[action] = TreeNode(self.game_core, new_state, self.reward, not self.player, net, self, action, terminal)\n",
    "        self.has_children = True        \n",
    "        return self.children[action]\n",
    "\n",
    "    def play_to_leaf(self):\n",
    "        # Проход от корня до создания нода (итерация MCTS)\n",
    "        #self.search_finished = False        \n",
    "        node = self\n",
    "        search_finished = False\n",
    "        while not search_finished:\n",
    "            #if node.player:\n",
    "                #print(\"Parent action \", node.parent_a)\n",
    "                #print(\"v: \", node.v)\n",
    "                #print(\"reward: \", node.reward)\n",
    "                #print(\"P: \", node.P)\n",
    "                #print(\"N: \", node.N)\n",
    "                #print(\"Q_totals: \", node.Q_totals)\n",
    "                #print(\"Q_star: \", node.Q_star)\n",
    "            node, search_finished = node.select_child()   \n",
    "        #print(\"State: \", node.state)\n",
    "        #print(\"Parent action \", node.parent_a)\n",
    "        #print(\"v: \", node.v)\n",
    "        #print(\"reward: \", node.reward)\n",
    "        #print(\"P: \", node.P)\n",
    "        #print(\"N: \", node.N)\n",
    "        #print(\"Q_totals: \", node.Q_totals)\n",
    "        #print(\"Q_star: \", node.Q_star)      \n",
    "        #print(\"Leaf made. \\n\")                       \n",
    "        return node\n",
    "\n",
    "    def backup(self, leaf):\n",
    "        # Обновление параметров\n",
    "        leaf.number_of_visits += 1        \n",
    "        value = leaf.v\n",
    "        reward = leaf.reward\n",
    "        parent_a = leaf.parent_a\n",
    "        node = leaf.parent\n",
    "        while node is not None:            \n",
    "            node.number_of_visits +=1            \n",
    "            if node.player:                \n",
    "                node.Q_totals[parent_a] += (value + reward)\n",
    "                node.Q_star[parent_a] = node.Q_totals[parent_a]/node.N[parent_a]\n",
    "                node.v = (node.P * node.Q_star).sum()\n",
    "                #node.Q[parent_a] = ((node.N[parent_a] - 1) * node.Q[parent_a] + value) / node.N[parent_a]\n",
    "                #node.Q[parent_a] = ((node.N[parent_a] - 1) * node.Q[parent_a] + np.log1p(movescore - node.movescore)) / node.N[parent_a]\n",
    "                value = node.v\n",
    "                reward = node.reward\n",
    "            parent_a = node.parent_a\n",
    "            node = node.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tiP5wjAxgZzA"
   },
   "outputs": [],
   "source": [
    "def tree_search(root, state, net, number):\n",
    "    # Поиск по дереву из состояния заданное число раз    \n",
    "    for i in range(number):\n",
    "        leaf = root.play_to_leaf() # Проход до нового листа (Select, Expand)        \n",
    "        leaf.backup(leaf) # Backup\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JNW2WTPOqn0Y",
    "outputId": "39658e3b-3efd-4af7-d684-56431286ef13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent action  -1\n",
      "v:  0\n",
      "reward:  0\n",
      "P:  [0.18260428 0.18798989 0.44349303 0.18591278]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Action:  0\n",
      "Parent action  3\n",
      "v:  0\n",
      "reward:  2.5649493574615367\n",
      "P:  [0.24417208 0.24504912 0.26472318 0.24605554]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Leaf made. \n",
      "\n",
      "Parent action  -1\n",
      "v:  0.46837073162118786\n",
      "reward:  0\n",
      "P:  [0.18260428 0.18798989 0.44349303 0.18591278]\n",
      "N:  [1. 0. 0. 0.]\n",
      "Q_totals:  [2.56494936 0.         0.         0.        ]\n",
      "Q_star:  [2.56494936 0.         0.         0.        ]\n",
      "Action:  0\n",
      "Parent action  4\n",
      "v:  0\n",
      "reward:  2.5649493574615367\n",
      "P:  [0.2449288  0.24533723 0.26479182 0.24494216]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Leaf made. \n",
      "\n",
      "Parent action  -1\n",
      "v:  0.46837073162118786\n",
      "reward:  0\n",
      "P:  [0.18260428 0.18798989 0.44349303 0.18591278]\n",
      "N:  [2. 0. 0. 0.]\n",
      "Q_totals:  [5.12989871 0.         0.         0.        ]\n",
      "Q_star:  [2.56494936 0.         0.         0.        ]\n",
      "Action:  0\n",
      "Parent action  0\n",
      "v:  0\n",
      "reward:  2.5649493574615367\n",
      "P:  [0.24413882 0.24450046 0.26594612 0.24541458]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Leaf made. \n",
      "\n",
      "Parent action  -1\n",
      "v:  0.46837073162118786\n",
      "reward:  0\n",
      "P:  [0.18260428 0.18798989 0.44349303 0.18591278]\n",
      "N:  [3. 0. 0. 0.]\n",
      "Q_totals:  [7.69484807 0.         0.         0.        ]\n",
      "Q_star:  [2.56494936 0.         0.         0.        ]\n",
      "Action:  0\n",
      "Parent action  4\n",
      "v:  0\n",
      "reward:  2.5649493574615367\n",
      "P:  [0.2449288  0.24533723 0.26479182 0.24494216]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Action:  0\n",
      "Parent action  4\n",
      "v:  0\n",
      "reward:  3.2188758248682006\n",
      "P:  [0.24337393 0.24990031 0.26298305 0.24374263]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Leaf made. \n",
      "\n",
      "Parent action  -1\n",
      "v:  0.5043618260065961\n",
      "reward:  0\n",
      "P:  [0.18260428 0.18798989 0.44349303 0.18591278]\n",
      "N:  [4. 0. 0. 0.]\n",
      "Q_totals:  [11.04819285  0.          0.          0.        ]\n",
      "Q_star:  [2.76204821 0.         0.         0.        ]\n",
      "Action:  2\n",
      "Parent action  0\n",
      "v:  0\n",
      "reward:  2.5649493574615367\n",
      "P:  [0.24637985 0.24677914 0.26013845 0.24670249]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Leaf made. \n",
      "\n",
      "Parent action  -1\n",
      "v:  1.6418989985567318\n",
      "reward:  0\n",
      "P:  [0.18260428 0.18798989 0.44349303 0.18591278]\n",
      "N:  [4. 0. 1. 0.]\n",
      "Q_totals:  [11.04819285  0.          2.56494936  0.        ]\n",
      "Q_star:  [2.76204821 0.         2.56494936 0.        ]\n",
      "Action:  2\n",
      "Parent action  1\n",
      "v:  0\n",
      "reward:  2.5649493574615367\n",
      "P:  [0.24547128 0.24710074 0.25919732 0.24823059]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Leaf made. \n",
      "\n",
      "Parent action  -1\n",
      "v:  1.6418989985567318\n",
      "reward:  0\n",
      "P:  [0.18260428 0.18798989 0.44349303 0.18591278]\n",
      "N:  [4. 0. 2. 0.]\n",
      "Q_totals:  [11.04819285  0.          5.12989871  0.        ]\n",
      "Q_star:  [2.76204821 0.         2.56494936 0.        ]\n",
      "Action:  2\n",
      "Parent action  4\n",
      "v:  0\n",
      "reward:  2.5649493574615367\n",
      "P:  [0.24614689 0.2471893  0.2595987  0.24706517]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Leaf made. \n",
      "\n",
      "Parent action  -1\n",
      "v:  1.6418989985567318\n",
      "reward:  0\n",
      "P:  [0.18260428 0.18798989 0.44349303 0.18591278]\n",
      "N:  [4. 0. 3. 0.]\n",
      "Q_totals:  [11.04819285  0.          7.69484807  0.        ]\n",
      "Q_star:  [2.76204821 0.         2.56494936 0.        ]\n",
      "Action:  2\n",
      "Parent action  3\n",
      "v:  0\n",
      "reward:  2.5649493574615367\n",
      "P:  [0.24700035 0.24700035 0.25899902 0.24700035]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Leaf made. \n",
      "\n",
      "Parent action  -1\n",
      "v:  1.6418989985567318\n",
      "reward:  0\n",
      "P:  [0.18260428 0.18798989 0.44349303 0.18591278]\n",
      "N:  [4. 0. 4. 0.]\n",
      "Q_totals:  [11.04819285  0.         10.25979743  0.        ]\n",
      "Q_star:  [2.76204821 0.         2.56494936 0.        ]\n",
      "Action:  2\n",
      "Parent action  1\n",
      "v:  0\n",
      "reward:  2.5649493574615367\n",
      "P:  [0.24547128 0.24710074 0.25919732 0.24823059]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Action:  0\n",
      "Parent action  2\n",
      "v:  0\n",
      "reward:  3.2188758248682006\n",
      "P:  [0.24382687 0.24653238 0.26443064 0.24521005]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Leaf made. \n",
      "\n",
      "Parent action  -1\n",
      "v:  1.7119834559723515\n",
      "reward:  0\n",
      "P:  [0.18260428 0.18798989 0.44349303 0.18591278]\n",
      "N:  [4. 0. 5. 0.]\n",
      "Q_totals:  [11.04819285  0.         13.61488837  0.        ]\n",
      "Q_star:  [2.76204821 0.         2.72297767 0.        ]\n",
      "Action:  2\n",
      "Parent action  3\n",
      "v:  0\n",
      "reward:  2.5649493574615367\n",
      "P:  [0.24700035 0.24700035 0.25899902 0.24700035]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Action:  0\n",
      "Parent action  5\n",
      "v:  0\n",
      "reward:  3.2188758248682006\n",
      "P:  [0.24480852 0.24820913 0.2613299  0.24565248]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Leaf made. \n",
      "\n",
      "Parent action  -1\n",
      "v:  1.7590702307748904\n",
      "reward:  0\n",
      "P:  [0.18260428 0.18798989 0.44349303 0.18591278]\n",
      "N:  [4. 0. 6. 0.]\n",
      "Q_totals:  [11.04819285  0.         16.97490119  0.        ]\n",
      "Q_star:  [2.76204821 0.         2.8291502  0.        ]\n",
      "Action:  2\n",
      "Parent action  3\n",
      "v:  0.7950634603324314\n",
      "reward:  2.5649493574615367\n",
      "P:  [0.24700035 0.24700035 0.25899902 0.24700035]\n",
      "N:  [1. 0. 0. 0.]\n",
      "Q_totals:  [3.21887582 0.         0.         0.        ]\n",
      "Q_star:  [3.21887582 0.         0.         0.        ]\n",
      "Action:  0\n",
      "Parent action  0\n",
      "v:  0\n",
      "reward:  3.2188758248682006\n",
      "P:  [0.244864   0.24827626 0.26199564 0.244864  ]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Leaf made. \n",
      "\n",
      "Parent action  -1\n",
      "v:  1.7927036413481323\n",
      "reward:  0\n",
      "P:  [0.18260428 0.18798989 0.44349303 0.18591278]\n",
      "N:  [4. 0. 7. 0.]\n",
      "Q_totals:  [11.04819285  0.         20.334914    0.        ]\n",
      "Q_star:  [2.76204821 0.         2.90498771 0.        ]\n",
      "Action:  2\n",
      "Parent action  3\n",
      "v:  0.7950634603324314\n",
      "reward:  2.5649493574615367\n",
      "P:  [0.24700035 0.24700035 0.25899902 0.24700035]\n",
      "N:  [2. 0. 0. 0.]\n",
      "Q_totals:  [6.43775165 0.         0.         0.        ]\n",
      "Q_star:  [3.21887582 0.         0.         0.        ]\n",
      "Action:  0\n",
      "Parent action  1\n",
      "v:  0\n",
      "reward:  3.2188758248682006\n",
      "P:  [0.24509117 0.24730554 0.2625121  0.24509117]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Leaf made. \n",
      "\n",
      "Parent action  -1\n",
      "v:  1.8179286992780639\n",
      "reward:  0\n",
      "P:  [0.18260428 0.18798989 0.44349303 0.18591278]\n",
      "N:  [4. 0. 8. 0.]\n",
      "Q_totals:  [11.04819285  0.         23.69492682  0.        ]\n",
      "Q_star:  [2.76204821 0.         2.96186585 0.        ]\n",
      "Action:  2\n",
      "Parent action  7\n",
      "v:  0\n",
      "reward:  2.5649493574615367\n",
      "P:  [0.24587752 0.24674925 0.25977397 0.24759929]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Leaf made. \n",
      "\n",
      "Parent action  -1\n",
      "v:  1.7983698436423603\n",
      "reward:  0\n",
      "P:  [0.18260428 0.18798989 0.44349303 0.18591278]\n",
      "N:  [4. 0. 9. 0.]\n",
      "Q_totals:  [11.04819285  0.         26.25987618  0.        ]\n",
      "Q_star:  [2.76204821 0.         2.91776402 0.        ]\n",
      "Action:  1\n",
      "Parent action  8\n",
      "v:  0\n",
      "reward:  3.367295829986474\n",
      "P:  [0.24624111 0.24624111 0.26127666 0.24624111]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Leaf made. \n",
      "\n",
      "Parent action  -1\n",
      "v:  2.431387430707046\n",
      "reward:  0\n",
      "P:  [0.18260428 0.18798989 0.44349303 0.18591278]\n",
      "N:  [4. 1. 9. 0.]\n",
      "Q_totals:  [11.04819285  3.36729583 26.25987618  0.        ]\n",
      "Q_star:  [2.76204821 3.36729583 2.91776402 0.        ]\n",
      "Action:  1\n",
      "Parent action  9\n",
      "v:  0\n",
      "reward:  3.367295829986474\n",
      "P:  [0.24647082 0.24647082 0.26058748 0.24647082]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Leaf made. \n",
      "\n",
      "Parent action  -1\n",
      "v:  2.431387430707046\n",
      "reward:  0\n",
      "P:  [0.18260428 0.18798989 0.44349303 0.18591278]\n",
      "N:  [4. 2. 9. 0.]\n",
      "Q_totals:  [11.04819285  6.73459166 26.25987618  0.        ]\n",
      "Q_star:  [2.76204821 3.36729583 2.91776402 0.        ]\n",
      "Action:  3\n",
      "Parent action  2\n",
      "v:  0\n",
      "reward:  3.367295829986474\n",
      "P:  [0.2439717  0.24854104 0.25869286 0.2487943 ]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Leaf made. \n",
      "\n",
      "Parent action  -1\n",
      "v:  3.0574107476482966\n",
      "reward:  0\n",
      "P:  [0.18260428 0.18798989 0.44349303 0.18591278]\n",
      "N:  [4. 2. 9. 1.]\n",
      "Q_totals:  [11.04819285  6.73459166 26.25987618  3.36729583]\n",
      "Q_star:  [2.76204821 3.36729583 2.91776402 3.36729583]\n",
      "Action:  3\n",
      "Parent action  3\n",
      "v:  0\n",
      "reward:  3.367295829986474\n",
      "P:  [0.2439884  0.24908435 0.25803047 0.24889675]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Leaf made. \n",
      "\n",
      "Parent action  -1\n",
      "v:  3.0574107476482966\n",
      "reward:  0\n",
      "P:  [0.18260428 0.18798989 0.44349303 0.18591278]\n",
      "N:  [4. 2. 9. 2.]\n",
      "Q_totals:  [11.04819285  6.73459166 26.25987618  6.73459166]\n",
      "Q_star:  [2.76204821 3.36729583 2.91776402 3.36729583]\n",
      "Action:  1\n",
      "Parent action  2\n",
      "v:  0\n",
      "reward:  3.367295829986474\n",
      "P:  [0.24628665 0.24628665 0.26114002 0.24628665]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Leaf made. \n",
      "\n",
      "Parent action  -1\n",
      "v:  3.0574107476482966\n",
      "reward:  0\n",
      "P:  [0.18260428 0.18798989 0.44349303 0.18591278]\n",
      "N:  [4. 3. 9. 2.]\n",
      "Q_totals:  [11.04819285 10.10188749 26.25987618  6.73459166]\n",
      "Q_star:  [2.76204821 3.36729583 2.91776402 3.36729583]\n",
      "Action:  3\n",
      "Parent action  5\n",
      "v:  0\n",
      "reward:  3.367295829986474\n",
      "P:  [0.24352816 0.2490187  0.25885156 0.24860156]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Leaf made. \n",
      "\n",
      "Parent action  -1\n",
      "v:  3.0574107476482966\n",
      "reward:  0\n",
      "P:  [0.18260428 0.18798989 0.44349303 0.18591278]\n",
      "N:  [4. 3. 9. 3.]\n",
      "Q_totals:  [11.04819285 10.10188749 26.25987618 10.10188749]\n",
      "Q_star:  [2.76204821 3.36729583 2.91776402 3.36729583]\n",
      "Action:  1\n",
      "Parent action  0\n",
      "v:  0\n",
      "reward:  3.367295829986474\n",
      "P:  [0.2463292  0.2463292  0.26101238 0.2463292 ]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Leaf made. \n",
      "\n",
      "Parent action  -1\n",
      "v:  3.0574107476482966\n",
      "reward:  0\n",
      "P:  [0.18260428 0.18798989 0.44349303 0.18591278]\n",
      "N:  [4. 4. 9. 3.]\n",
      "Q_totals:  [11.04819285 13.46918332 26.25987618 10.10188749]\n",
      "Q_star:  [2.76204821 3.36729583 2.91776402 3.36729583]\n",
      "Action:  3\n",
      "Parent action  5\n",
      "v:  0\n",
      "reward:  3.367295829986474\n",
      "P:  [0.24352816 0.2490187  0.25885156 0.24860156]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Action:  0\n",
      "Parent action  4\n",
      "v:  0\n",
      "reward:  2.5649493574615367\n",
      "P:  [0.24605626 0.24865416 0.25737658 0.24791296]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Leaf made. \n",
      "\n",
      "Parent action  -1\n",
      "v:  3.086442765561442\n",
      "reward:  0\n",
      "P:  [0.18260428 0.18798989 0.44349303 0.18591278]\n",
      "N:  [4. 4. 9. 4.]\n",
      "Q_totals:  [11.04819285 13.46918332 26.25987618 14.09382071]\n",
      "Q_star:  [2.76204821 3.36729583 2.91776402 3.52345518]\n",
      "Action:  3\n",
      "Parent action  1\n",
      "v:  0\n",
      "reward:  3.367295829986474\n",
      "P:  [0.24290076 0.24893856 0.25926095 0.24889973]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Leaf made. \n",
      "\n",
      "Parent action  -1\n",
      "v:  3.080636361978813\n",
      "reward:  0\n",
      "P:  [0.18260428 0.18798989 0.44349303 0.18591278]\n",
      "N:  [4. 4. 9. 5.]\n",
      "Q_totals:  [11.04819285 13.46918332 26.25987618 17.46111654]\n",
      "Q_star:  [2.76204821 3.36729583 2.91776402 3.49222331]\n",
      "Action:  3\n",
      "Parent action  1\n",
      "v:  0\n",
      "reward:  3.367295829986474\n",
      "P:  [0.24290076 0.24893856 0.25926095 0.24889973]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Action:  0\n",
      "Parent action  4\n",
      "v:  0\n",
      "reward:  2.5649493574615367\n",
      "P:  [0.24341182 0.24984041 0.25775838 0.24898943]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Leaf made. \n",
      "\n",
      "Parent action  -1\n",
      "v:  3.096070241653795\n",
      "reward:  0\n",
      "P:  [0.18260428 0.18798989 0.44349303 0.18591278]\n",
      "N:  [4. 4. 9. 6.]\n",
      "Q_totals:  [11.04819285 13.46918332 26.25987618 21.45144052]\n",
      "Q_star:  [2.76204821 3.36729583 2.91776402 3.57524009]\n",
      "Action:  3\n",
      "Parent action  4\n",
      "v:  0\n",
      "reward:  3.367295829986474\n",
      "P:  [0.24471854 0.24722661 0.2594105  0.2486444 ]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Leaf made. \n",
      "\n",
      "Parent action  -1\n",
      "v:  3.090547456795867\n",
      "reward:  0\n",
      "P:  [0.18260428 0.18798989 0.44349303 0.18591278]\n",
      "N:  [4. 4. 9. 7.]\n",
      "Q_totals:  [11.04819285 13.46918332 26.25987618 24.81873635]\n",
      "Q_star:  [2.76204821 3.36729583 2.91776402 3.54553376]\n",
      "Action:  1\n",
      "Parent action  4\n",
      "v:  0\n",
      "reward:  3.367295829986474\n",
      "P:  [0.2462977  0.2462977  0.25995398 0.24745059]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Leaf made. \n",
      "\n",
      "Parent action  -1\n",
      "v:  3.090547456795867\n",
      "reward:  0\n",
      "P:  [0.18260428 0.18798989 0.44349303 0.18591278]\n",
      "N:  [4. 5. 9. 7.]\n",
      "Q_totals:  [11.04819285 16.83647915 26.25987618 24.81873635]\n",
      "Q_star:  [2.76204821 3.36729583 2.91776402 3.54553376]\n",
      "Action:  3\n",
      "Parent action  5\n",
      "v:  0.6246373910331642\n",
      "reward:  3.367295829986474\n",
      "P:  [0.24352816 0.2490187  0.25885156 0.24860156]\n",
      "N:  [1. 0. 0. 0.]\n",
      "Q_totals:  [2.56494936 0.         0.         0.        ]\n",
      "Q_star:  [2.56494936 0.         0.         0.        ]\n",
      "Action:  0\n",
      "Parent action  3\n",
      "v:  0\n",
      "reward:  2.5649493574615367\n",
      "P:  [0.2471936  0.24758247 0.2574625  0.24776134]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Leaf made. \n",
      "\n",
      "Parent action  -1\n",
      "v:  3.1009213771089934\n",
      "reward:  0\n",
      "P:  [0.18260428 0.18798989 0.44349303 0.18591278]\n",
      "N:  [4. 5. 9. 8.]\n",
      "Q_totals:  [11.04819285 16.83647915 26.25987618 28.81066957]\n",
      "Q_star:  [2.76204821 3.36729583 2.91776402 3.6013337 ]\n",
      "Action:  3\n",
      "Parent action  1\n",
      "v:  0.6230281456770431\n",
      "reward:  3.367295829986474\n",
      "P:  [0.24290076 0.24893856 0.25926095 0.24889973]\n",
      "N:  [1. 0. 0. 0.]\n",
      "Q_totals:  [2.56494936 0.         0.         0.        ]\n",
      "Q_star:  [2.56494936 0.         0.         0.        ]\n",
      "Action:  0\n",
      "Parent action  4\n",
      "v:  0\n",
      "reward:  2.5649493574615367\n",
      "P:  [0.24341182 0.24984041 0.25775838 0.24898943]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Action:  0\n",
      "Parent action  1\n",
      "v:  0\n",
      "reward:  2.833213344056216\n",
      "P:  [0.24615563 0.24760172 0.25880998 0.24743266]\n",
      "N:  [0. 0. 0. 0.]\n",
      "Q_totals:  [0. 0. 0. 0.]\n",
      "Q_star:  [0. 0. 0. 0.]\n",
      "Leaf made. \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.TreeNode at 0x7f7ec50027f0>"
      ]
     },
     "execution_count": 100,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = np.array([[2,4,4,8],[2,8,4,8],[0,4,2,2],[4,8,0,0]])\n",
    "root = TreeNode(Game_Core_2048(), state, 0, True, net)\n",
    "tree_search(root, state, net, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Je4ASjGzGje7"
   },
   "outputs": [],
   "source": [
    "def self_play(net, dataset_size=4096, num_MCTS=999, random_start=0., max_number=128):\n",
    "    # Self-play по алгоритму Alpha Zero для создания датасета для обучения сети.\n",
    "    dataset = []    \n",
    "    env = Env2048(gui=False, inv_move_tolerance=1)\n",
    "\n",
    "    total_score = 0\n",
    "    total_num_moves = 0\n",
    "    total_inv_moves = 0\n",
    "    mean_score = 0.\n",
    "    mean_num_moves = 0.\n",
    "    total_max_num_reached = 0\n",
    "    total_num_moves = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    i = 0\n",
    "    while len(dataset) < dataset_size:\n",
    "        i += 1\n",
    "        print(\"Simulation {}, dataset length {}\".format(i, len(dataset)))\n",
    "        env.reset(random_start, max_number)\n",
    "        terminal = False     \n",
    "        tau = 1. # Температура\n",
    "        num_moves = 0    \n",
    "        dataset_sim = []\n",
    "        #dataset_sim_double = []\n",
    "        state = env.get_state()\n",
    "        root = TreeNode(Game_Core_2048(), state, 0, True, net)\n",
    "        print(\"Max number started: \", np.max(state))\n",
    "        while not terminal:        \n",
    "            state = env.get_state()            \n",
    "            pi, possible_moves = tree_search(root, state, net, num_MCTS).get_Q_star() # Создание policy\n",
    "            direction_n = np.random.choice(np.arange(len(pi)), p=softmax(pi*possible_moves))\n",
    "            direction = dir_to_ohe(direction_n)\n",
    "            cur_score = env.game_core.score\n",
    "            # Заполнение датасета с аугментацией\n",
    "            dataset_sim.append([state_to_ohe(state), pi]) \n",
    "            #dataset_sim_double.append([state_to_ohe(state*2), pi, cur_score*2]) \n",
    "            h_flip_pi = pi.copy()\n",
    "            h_flip_pi[0], h_flip_pi[2] = h_flip_pi[2], h_flip_pi[0] \n",
    "            dataset_sim.append([state_to_ohe(np.fliplr(state)), h_flip_pi]) \n",
    "            #dataset_sim_double.append([state_to_ohe(np.fliplr(state*2)), h_flip_pi, cur_score*2]) \n",
    "            v_flip_pi = pi.copy()\n",
    "            v_flip_pi[1], v_flip_pi[3] = v_flip_pi[3], v_flip_pi[1] \n",
    "            dataset_sim.append([state_to_ohe(np.flipud(state)), v_flip_pi]) \n",
    "            #dataset_sim_double.append([state_to_ohe(np.flipud(state*2)), v_flip_pi, cur_score*2]) \n",
    "            rot90_state = np.rot90(state)\n",
    "            rot90_pi = np.roll(pi, -1)\n",
    "            dataset_sim.append([state_to_ohe(rot90_state), rot90_pi]) \n",
    "            #dataset_sim_double.append([state_to_ohe(rot90_state*2), rot90_pi, cur_score*2]) \n",
    "            h_flip_pi = rot90_pi.copy()\n",
    "            h_flip_pi[0], h_flip_pi[2] = h_flip_pi[2], h_flip_pi[0] \n",
    "            dataset_sim.append([state_to_ohe(np.fliplr(rot90_state)), h_flip_pi]) \n",
    "            #dataset_sim_double.append([state_to_ohe(np.fliplr(rot90_state*2)), h_flip_pi, cur_score*2]) \n",
    "            v_flip_pi = rot90_pi.copy()\n",
    "            v_flip_pi[1], v_flip_pi[3] = v_flip_pi[3], v_flip_pi[1] \n",
    "            dataset_sim.append([state_to_ohe(np.flipud(rot90_state)), v_flip_pi]) \n",
    "            #dataset_sim_double.append([state_to_ohe(np.flipud(rot90_state*2)), v_flip_pi, cur_score*2]) \n",
    "            dataset_sim.append([state_to_ohe(np.rot90(state, 2)), np.roll(pi, -2)]) \n",
    "            #dataset_sim_double.append([state_to_ohe(np.rot90(state*2, 2)), np.roll(pi, -2), cur_score*2]) \n",
    "            dataset_sim.append([state_to_ohe(np.rot90(state, 3)), np.roll(pi, -3)]) \n",
    "            #dataset_sim_double.append([state_to_ohe(np.rot90(state*2, 3)), np.roll(pi, -3), cur_score*2]) \n",
    "\n",
    "            _, _, reward, new_state, terminal = env.act(direction, ohe_state=False)      \n",
    "            new_root = root.find_node(direction_n, new_state) # Reuse дерева\n",
    "            if new_root:                \n",
    "                root = new_root\n",
    "                root.make_root()\n",
    "            else:                \n",
    "                root = TreeNode(Game_Core_2048(), new_state, 0, True, net)            \n",
    "            num_moves += 1\n",
    "            if num_moves > 30:\n",
    "                tau = 0.1\n",
    "            if reward == param_inv_move_reward:\n",
    "                total_inv_moves += 1                \n",
    "\n",
    "        cur_score = env.game_core.score\n",
    "        #for data in dataset_sim:\n",
    "         #   data[2] = np.log1p(cur_score - data[2])\n",
    "        #for data in dataset_sim_double:\n",
    "         #   data[2] = np.log1p(cur_score*2 - data[2])\n",
    "        dataset += dataset_sim\n",
    "        #dataset += dataset_sim_double\n",
    "        with open(os.path.join(PATH, \"dataset.pickle\"), \"wb\") as f:\n",
    "            pickle.dump(dataset, f)\n",
    "        total_score += cur_score\n",
    "        total_num_moves += num_moves\n",
    "        total_max_num_reached += np.max(new_state)\n",
    "        print(\"Max number reached: {}, moves made: {}, score: {}\".format(np.max(new_state), num_moves, cur_score))\n",
    "        print(\"Time: \", (time.time() - start_time)/60.)\n",
    "\n",
    "    mean_score = total_score/i\n",
    "    mean_num_moves = total_num_moves/i\n",
    "    mean_max_num_reached = total_max_num_reached/i\n",
    "    mean_inv_moves = total_inv_moves/total_num_moves\n",
    "        \n",
    "    return dataset, (mean_score, mean_num_moves, mean_max_num_reached, mean_inv_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kTRXboUek9xB"
   },
   "outputs": [],
   "source": [
    "# Сеть по образцу из статьи.\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.action_size = param_n_actions\n",
    "        self.conv1 = nn.Conv2d(14, 128, 3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "\n",
    "    def forward(self, s):        \n",
    "        s = F.relu(self.bn1(self.conv1(s)))\n",
    "        return s\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, inplanes=128, planes=128, stride=1, downsample=None):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += residual\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ConvBlockWider(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvBlockWider, self).__init__()        \n",
    "        self.conv1 = nn.Conv2d(14, 256, 3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "\n",
    "    def forward(self, s):        \n",
    "        s = F.relu(self.bn1(self.conv1(s)))\n",
    "        return s\n",
    "\n",
    "class ResBlockWider(nn.Module):\n",
    "    def __init__(self, inplanes=256, planes=256, stride=1, downsample=None):\n",
    "        super(ResBlockWider, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += residual\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class OutBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OutBlock, self).__init__()\n",
    "        #self.conv = nn.Conv2d(256, 14, kernel_size=1) \n",
    "        #self.bn = nn.BatchNorm2d(14)\n",
    "        #self.fc1 = nn.Linear(14*4*4, 32)\n",
    "        #self.fc2 = nn.Linear(32, 1)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(256, 32, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        #self.logsoftmax = nn.LogSoftmax(dim=-1)\n",
    "        self.fc = nn.Linear(32*4*4, param_n_actions)\n",
    "    \n",
    "    def forward(self,s):\n",
    "        #v = F.relu(self.bn(self.conv(s))) \n",
    "        #v = v.view(-1, 14*4*4)  \n",
    "        #v = F.relu(self.fc1(v))\n",
    "        #v = F.relu(self.fc2(v))\n",
    "        \n",
    "        q = F.relu(self.bn1(self.conv1(s))) \n",
    "        q = q.view(-1, 32*4*4)\n",
    "        q = F.relu(self.fc(q))\n",
    "        #p_probas = self.logsoftmax(p_logits).exp()        \n",
    "        return q\n",
    "\n",
    "class Alpha2048net(nn.Module):\n",
    "    def __init__(self, device=\"cpu\"):\n",
    "        super(Alpha2048net, self).__init__()\n",
    "        #self.conv = ConvBlock()\n",
    "        self.conv = ConvBlockWider()\n",
    "        #for block in range(19):\n",
    "        #for block in range(3):\n",
    "            #setattr(self, \"res_%i\" % block, ResBlock())\n",
    "            #setattr(self, \"res_%i\" % block, ResBlockWider())\n",
    "        self.resblocks = nn.Sequential(ResBlockWider(),\n",
    "                                       ResBlockWider(),\n",
    "                                       ResBlockWider())\n",
    "        self.outblock = OutBlock()\n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self,s):\n",
    "        s = self.conv(s)\n",
    "        #for block in range(19):\n",
    "        #for block in range(3):\n",
    "         #   s = getattr(self, \"res_%i\" % block)(s)\n",
    "        s = self.resblocks(s)\n",
    "        s = self.outblock(s)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "vTUq_gj7doxn",
    "outputId": "da1d4606-2342-4e24-d724-53fc34fef1a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = Alpha2048net(device=device)\n",
    "net_optim = optim.Adam(net.parameters(), lr=1e-6, weight_decay=1e-5)\n",
    "net.eval()\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "HL9wQIajMNC6",
    "outputId": "ac9d4766-2ab5-49e0-f612-6aa12e6f9ec7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(os.path.join(PATH, \"net_wider3_weights_dataset_22-wider3-S-M2-32768.pth\"), map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 665
    },
    "colab_type": "code",
    "id": "BwV3yBO2NyiX",
    "outputId": "5866bc02-8914-44b0-ca22-49fa514abcd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation 1, dataset length 0\n",
      "Max number started:  128\n",
      "Max number reached: 512, moves made: 21, score: 1156\n",
      "Time:  6.0157932122548425\n",
      "Simulation 2, dataset length 168\n",
      "Max number started:  2\n",
      "Max number reached: 64, moves made: 42, score: 360\n",
      "Time:  17.47962795495987\n",
      "Simulation 3, dataset length 504\n",
      "Max number started:  2\n",
      "Max number reached: 4, moves made: 4, score: 4\n",
      "Time:  18.488186275959016\n",
      "Simulation 4, dataset length 536\n",
      "Max number started:  128\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-3ffe7105aca0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_play\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_MCTS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_number\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dataset.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-108-378bcc0210a5>\u001b[0m in \u001b[0;36mself_play\u001b[0;34m(net, dataset_size, num_MCTS, random_start, max_number)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_MCTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_Q_star\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Создание policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mdirection_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mdirection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdir_to_ohe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-107-b44399f0fc7c>\u001b[0m in \u001b[0;36mtree_search\u001b[0;34m(root, state, net, number)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#root = TreeNode(Game_Core_2048(), state, 0, True, net)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mleaf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_to_leaf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Проход до нового листа (Select, Expand)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m#p, v = net(leaf.state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m#if not leaf.terminal:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-106-c77d51fdd7d0>\u001b[0m in \u001b[0;36mplay_to_leaf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0;31m#print(\"Q_totals: \", node.Q_totals)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;31m#print(\"Q_star: \", node.Q_star)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_finished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;31m#print(\"State: \", node.state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m#print(\"Parent action \", node.parent_a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-106-c77d51fdd7d0>\u001b[0m in \u001b[0;36mselect_child\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-106-c77d51fdd7d0>\u001b[0m in \u001b[0;36mexpand\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# Создание нового нода\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvalid_move\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_from_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minvalid_move\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mterminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-982f9e6e1cfc>\u001b[0m in \u001b[0;36mmove_from_state\u001b[0;34m(self, state, direction, moves_dict)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmovescore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0;31m#self.place_random_number()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mmoves_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mstate_gameover\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_gameover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoves_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstate_gameover\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-982f9e6e1cfc>\u001b[0m in \u001b[0;36mfind_all_moves\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_offset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mmoves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_gb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malready_summed_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m                 \u001b[0mdir_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmoves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmoves_available\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-982f9e6e1cfc>\u001b[0m in \u001b[0;36mfind_moves\u001b[0;34m(self, gameboard, direction, already_summed_mask, offset)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mcheck_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_inds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdel_axis\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdel_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mcur_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdel_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0mcur_values_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_values_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdel_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdel_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Генерация датасета из игр.\n",
    "#start_time = time.time()\n",
    "dataset, stats = self_play(net, dataset_size=1024, num_MCTS=512, random_start=0.5, max_number=128)\n",
    "\n",
    "with open(os.path.join(PATH, \"dataset.pickle\"), \"wb\") as f:\n",
    "    pickle.dump(dataset, f)\n",
    "\n",
    "print(\"Mean score {:.3f}, mean number of moves {:.3f}, mean max number {:.3f}, invalid moves ratio {:.3f}\".format(*stats))\n",
    "#print(\"Time: \", (time.time() - start_time)/60.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VQ6w1lyB92-j"
   },
   "outputs": [],
   "source": [
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uh-w-hZuuVLK"
   },
   "outputs": [],
   "source": [
    "def train(epoch, net, optim, loader, log):\n",
    "    net.train()\n",
    "    mean_loss = 0.\n",
    "    for data, label in loader:\n",
    "        optim.zero_grad()\n",
    "        data = data.to(device)\n",
    "        label_pi = label.to(device)        \n",
    "        \n",
    "        p = net(data)\n",
    "                \n",
    "        loss = F.mse_loss(p, label_pi) #torch.sum((-label_pi * (1e-8 + p).log()), 1) # Policy loss        \n",
    "        #loss_v =  ((label_z - v)**2).view(-1) # Value loss        \n",
    "        #loss = loss_p.mean() #(loss_p + loss_v).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        mean_loss += loss.item()        \n",
    "    \n",
    "    mean_loss /= len(loader)\n",
    "    log[\"train\"].append(mean_loss)\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch {} loss: {:.3f}\".format(epoch, mean_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RrHDEut-Rn9a"
   },
   "outputs": [],
   "source": [
    "def validate(epoch, net, loader, log):\n",
    "    net.eval()\n",
    "    mean_loss = 0.\n",
    "    for data, label in loader:        \n",
    "        data = data.to(device)\n",
    "        label_pi = label.to(device)                \n",
    "        \n",
    "        p = net(data)        \n",
    "        \n",
    "        loss = F.mse_loss(p, label_pi) #loss_p = torch.sum((-label_pi * (1e-8 + p).log()), 1)        \n",
    "        #loss_v =  ((label_z - v)**2).view(-1)\n",
    "        #loss = loss_p.mean() #(loss_p + loss_v).mean()\n",
    "        mean_loss += loss.item()        \n",
    "    \n",
    "    mean_loss /= len(loader)\n",
    "    log[\"val\"].append(mean_loss)\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch {} validation loss: {:.3f}\".format(epoch, mean_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K0BiQ1VXn1SZ"
   },
   "outputs": [],
   "source": [
    "# Создание pytorch loaders\n",
    "#np.random.shuffle(dataset)\n",
    "#split_idx = int(len(dataset)*.75)\n",
    "dataset_train = dataset#[:split_idx]\n",
    "#dataset_test = dataset[split_idx:]\n",
    "dataset_train_T = list(zip(*dataset_train))\n",
    "#dataset_test_T = list(zip(*dataset_test))\n",
    "\n",
    "data_state_train = torch.Tensor(dataset_train_T[0])\n",
    "labels_train = torch.Tensor(dataset_train_T[1])\n",
    "#data_z_train = torch.Tensor(dataset_train_T[2]).view(-1,1)\n",
    "#labels_train = torch.cat((data_pi_train, data_z_train), dim=1)\n",
    "#data_state_test = torch.Tensor(dataset_test_T[0])\n",
    "#data_pi_test = torch.Tensor(dataset_test_T[1])\n",
    "#data_z_test = torch.Tensor(dataset_test_T[2]).view(-1,1)\n",
    "#labels_test = torch.cat((data_pi_test, data_z_test), dim=1)\n",
    "\n",
    "tensor_dataset_train = TensorDataset(data_state_train, labels_train)\n",
    "loader_train = DataLoader(tensor_dataset_train, batch_size=param_batch_size, shuffle=True)\n",
    "\n",
    "#tensor_dataset_test = TensorDataset(data_state_test, labels_test)\n",
    "#loader_test = DataLoader(tensor_dataset_test, batch_size=param_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "9ZHJa1IN3FFR",
    "outputId": "bbfea061-bdf9-4db4-9743-fa304549a01b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 3.823\n",
      "Epoch 10 loss: 2.897\n",
      "\n"
     ]
    }
   ],
   "source": [
    "losses_log  = {\"train\" : [], \"val\" : []}\n",
    "for epoch in range(11):\n",
    "    train(epoch, net, net_optim, loader_train, losses_log)\n",
    "    #validate(epoch, net, loader_test, losses_log)\n",
    "torch.save(net.state_dict(), os.path.join(PATH, \"DaQN_weights_001.pth\"))\n",
    "net.eval()\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "tsqHaWTRnmu9",
    "outputId": "a05345e3-87fe-42c5-be22-cebb2a6079ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "536\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(PATH, \"dataset.pickle\"), \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "0YUI7I2K90W4",
    "outputId": "b75dde77-47ec-430e-9668-54b4e18be9df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(os.path.join(PATH, \"net_weights_dataset_5-2048.pth\"), map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "nYC-DR78bA71",
    "outputId": "4b3250f4-c6a8-42f9-90e9-8b4d746d0272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DAlphaQN2048.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
