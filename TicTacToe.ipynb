{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "6p2ANr9STFyL",
    "outputId": "a1bf2dfc-016f-43b7-fcf9-6a849e1b5fdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "Ts1CG_YjWDW6",
    "outputId": "4bfd33f4-0992-46a2-da36-ac5414d22c01"
   },
   "outputs": [],
   "source": [
    "PATH = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oZ8oaTqaTFyW"
   },
   "outputs": [],
   "source": [
    "# Игровое поле\n",
    "class GameBoard:\n",
    "    def __init__(self):        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.board = np.zeros((3, 3)).astype(int)\n",
    "        self.done = False\n",
    "        self.X_reward = 0.\n",
    "        self.O_reward = 0.\n",
    "    \n",
    "    def set_sign(self, coords, sign):\n",
    "        # Выполняет ход, если возможно\n",
    "        assert(type(coords) is tuple and len(coords)==2)\n",
    "        if self.board[coords] != 0:\n",
    "            return False\n",
    "        if sign==1 or sign==-1:\n",
    "            self.board[coords] = sign\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self.board\n",
    "    \n",
    "    def get_actions(self):\n",
    "        # Возвращает допустимые действия\n",
    "        return np.argwhere(self.board==0)\n",
    "    \n",
    "    def check_win(self, sign, state=None):\n",
    "        # Проверяет выигрыш игрока\n",
    "        if state is None:\n",
    "            state = self.board\n",
    "        win = False\n",
    "        if np.any(np.sum(state, axis=0) == 3*sign) or np.any(np.sum(state, axis=1) == 3*sign) \\\n",
    "        or np.trace(state) == 3*sign or np.trace(np.rot90(state)) == 3*sign :\n",
    "            win = True        \n",
    "        return win\n",
    "    \n",
    "    def check_draw(self):\n",
    "        # Проверяет ничью\n",
    "        if 0 not in self.board:\n",
    "            return True\n",
    "    \n",
    "    def check_done(self):\n",
    "        # Проверяет завершение партии\n",
    "        if self.check_win(1):\n",
    "            #print(\"X won\")\n",
    "            self.X_reward = 1.\n",
    "            self.O_reward = 0.\n",
    "            self.done = True\n",
    "        if self.check_win(-1):\n",
    "            #print(\"O won\")\n",
    "            self.O_reward = 1.\n",
    "            self.X_reward = 0.\n",
    "            self.done = True\n",
    "        if self.check_draw():\n",
    "            #print(\"Draw\")\n",
    "            self.X_reward = 0.1\n",
    "            self.O_reward = 0.1\n",
    "            self.done = True        \n",
    "        return self.done\n",
    "    \n",
    "    def get_reward(self, player):\n",
    "        # Возвращает награду\n",
    "        if player.sign == 1:\n",
    "            return self.X_reward\n",
    "        if player.sign == -1:\n",
    "            return self.O_reward\n",
    "    \n",
    "    def print(self):\n",
    "        # Печатает поле\n",
    "        sign = lambda x: \"X\" if x == 1 else \"O\" if x == -1 else \" \"\n",
    "        print(\"  0 1 2 \")\n",
    "        print(\"  - - - \")\n",
    "        print(\"0|{}|{}|{}|\".format(sign(self.board[0,0]), sign(self.board[0,1]), sign(self.board[0,2])))\n",
    "        print(\"  - - - \")\n",
    "        print(\"1|{}|{}|{}|\".format(sign(self.board[1,0]), sign(self.board[1,1]), sign(self.board[1,2])))\n",
    "        print(\"  - - - \")\n",
    "        print(\"2|{}|{}|{}|\".format(sign(self.board[2,0]), sign(self.board[2,1]), sign(self.board[2,2])))\n",
    "        print(\"  - - - \")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q-rkVpsuTFyd"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Класс игрока.\n",
    "Включает бот для игры, стремящийся поставить три в ряд и\n",
    "блокирующий такие попытки оппонента. Пытается вилки делать.\n",
    "Простенький, но играть может.\n",
    "\"\"\"\n",
    "class Player:\n",
    "    def __init__(self, board, side=\"X\"):\n",
    "        self.board = board\n",
    "        self.sign = 1 if side==\"X\" else -1 if side==\"O\" else 0\n",
    "            \n",
    "    def __check_danger(self, coords, sign, state):  \n",
    "        # Проверяет опасную ситуацию      \n",
    "        c0 = coords[0]\n",
    "        c1 = coords[1]        \n",
    "        if (state[c0, :]==-sign).sum()==2:\n",
    "            return True\n",
    "        if (state[:, c1]==-sign).sum()==2:\n",
    "            return True        \n",
    "        if c0==1 and c1==1:\n",
    "            if np.trace(state) == -2*sign or np.trace(np.rot90(state))==-2*sign:\n",
    "                return True\n",
    "        elif c0==c1:            \n",
    "            if np.trace(state) == -2*sign:\n",
    "                return True\n",
    "        elif np.abs(c0-c1)==2:            \n",
    "            if np.trace(np.rot90(state))==-2*sign:\n",
    "                return True        \n",
    "        return False\n",
    "    \n",
    "    def __find_danger(self, sign, state):         \n",
    "        return np.argwhere(state==0)[np.array([self.__check_danger(c, sign, state) for c in self.get_actions(state)])]\n",
    "    \n",
    "    def get_actions(self, state):\n",
    "        return np.argwhere(state==0)\n",
    "    \n",
    "    def make_move(self, coords):\n",
    "        # Для совершения хода вручную\n",
    "        if self.board.check_done():\n",
    "            return False\n",
    "        if self.board.set_sign(coords, self.sign):\n",
    "            self.board.check_done()\n",
    "            return True\n",
    "        print(\"Invalid move\")\n",
    "        return False\n",
    "    \n",
    "    def act(self):\n",
    "        # Совершение хода ботом\n",
    "        if self.board.check_done():\n",
    "            return False\n",
    "        wins = self.__find_danger(-self.sign, self.board.get_state())\n",
    "        if len(wins) > 0:\n",
    "            self.make_move(tuple(wins[0]))\n",
    "            return True\n",
    "        dangers = self.__find_danger(self.sign, self.board.get_state())\n",
    "        if len(dangers) > 0:\n",
    "            self.make_move(tuple(dangers[0]))\n",
    "            return True\n",
    "        actions = self.get_actions(self.board.get_state())\n",
    "        if [1, 1] in actions.tolist():            \n",
    "            self.make_move((1, 1))\n",
    "            return True\n",
    "        else:\n",
    "            potential = 0\n",
    "            action = actions[np.random.randint(len(actions))]\n",
    "            for a in actions:                \n",
    "                test_state = self.board.get_state().copy()\n",
    "                test_state[tuple(a)] = self.sign\n",
    "                if len(self.__find_danger(sign=-self.sign, state=test_state)) > potential:\n",
    "                    potential = len(self.__find_danger(sign=-self.sign, state=test_state))\n",
    "                    action = a\n",
    "                    #print(a)\n",
    "            self.make_move(tuple(action))        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "colab_type": "code",
    "id": "lMAF3hXtTFyi",
    "outputId": "2bc114a7-3958-45cd-82ab-b6ece4d3c84f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 1 2 \n",
      "  - - - \n",
      "0| | | |\n",
      "  - - - \n",
      "1| | | |\n",
      "  - - - \n",
      "2| | | |\n",
      "  - - - \n"
     ]
    }
   ],
   "source": [
    "gb = GameBoard()\n",
    "pla = Player(gb)\n",
    "bot = Player(gb, \"O\")\n",
    "gb.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "colab_type": "code",
    "id": "UxRuHEIiTFyn",
    "outputId": "6d299c4c-b6d4-4246-e4a5-127594da078d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 1 2 \n",
      "  - - - \n",
      "0|X|X| |\n",
      "  - - - \n",
      "1| |X| |\n",
      "  - - - \n",
      "2|O| |O|\n",
      "  - - - \n",
      "0.0\n",
      "  0 1 2 \n",
      "  - - - \n",
      "0|X|X| |\n",
      "  - - - \n",
      "1| |X| |\n",
      "  - - - \n",
      "2|O|O|O|\n",
      "  - - - \n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Можно поиграть вручную, задавая коордтнаты в make_move()\n",
    "if(pla.make_move((0,1))):\n",
    "    gb.print()\n",
    "    print(gb.get_reward(pla))\n",
    "    if(bot.act()):\n",
    "        gb.print()\n",
    "        print(gb.get_reward(bot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0nTxM9eETFyt"
   },
   "outputs": [],
   "source": [
    "# Класс таблицы Q(s,a)\n",
    "class QTable:\n",
    "    def __init__(self):\n",
    "        self.__generate_positions()\n",
    "        self.all_actions = np.array([p for p in itertools.product(range(3), repeat=2)])\n",
    "        self.__generate_table()    \n",
    "    \n",
    "    def __generate_positions(self):\n",
    "        # Создает список всех возможных позиций.\n",
    "        positions = []\n",
    "        for i, c in enumerate(itertools.product(range(3), repeat=9)):\n",
    "            c = np.array(c)-1  \n",
    "            # Количество крестиков и ноликов на доске должно быть равно\n",
    "            if (c==1).sum() == (c==-1).sum():\n",
    "                c = c.reshape([3,3])                \n",
    "                # Не должно быть выигрышных позиций\n",
    "                if not GameBoard().check_win(1, c) and not GameBoard().check_win(-1, c):\n",
    "                    positions.append(c)\n",
    "        self.positions = np.array(positions).astype(int)\n",
    "    \n",
    "    def __generate_table(self):\n",
    "        # Создает q-таблицу с нулями\n",
    "        self.table = np.zeros((9, len(self.positions)))        \n",
    "    \n",
    "    def patois(self, plaa):\n",
    "        # Возвращает индексы доступных игроку действий\n",
    "        return np.array([np.where(np.all(self.all_actions==plaa[i], axis=1))[0][0] for i in range(len(plaa))])\n",
    "    \n",
    "    def atoi(self, action):\n",
    "        # Возвращает индекс действия\n",
    "        return np.where(np.all(self.all_actions==action, axis=1))[0][0]\n",
    "    \n",
    "    def stoi(self, state):\n",
    "        # Возвращает индекс состояния\n",
    "        return np.where(np.all(self.positions == state, axis=(1,2)))[0][0]\n",
    "    \n",
    "    def get_states(self):\n",
    "        # Возвращает все позиции\n",
    "        return self.positions\n",
    "    \n",
    "    def get_table(self):\n",
    "        # Возвращает таблицу\n",
    "        return self.table\n",
    "    \n",
    "    def get_all_actions(self):\n",
    "        # Возвращает все действия\n",
    "        return self.all_actions\n",
    "    \n",
    "    def get_Q(self, state, action):\n",
    "        # Возвращает Q(s,a)\n",
    "        si = self.stoi(state)\n",
    "        ai = self.atoi(action)\n",
    "        return self.table[ai, si]\n",
    "    \n",
    "    def set_Q(self, state, action, value):\n",
    "        # Устанавливает Q(s,a)\n",
    "        si = self.stoi(state)\n",
    "        ai = self.atoi(action)        \n",
    "        self.table[ai, si] = value\n",
    "        \n",
    "    def get_Qs(self, state):\n",
    "        # Возвращает значения Q для всех действий при состоянии\n",
    "        si = self.stoi(state)        \n",
    "        return self.table[:, si]\n",
    "    \n",
    "    def save(self):\n",
    "        with open(os.path.join(PATH, \"qtable.pickle\"), \"wb\") as f:\n",
    "            pickle.dump(self.table, f)\n",
    "            \n",
    "    def load(self):\n",
    "        with open(os.path.join(PATH, \"qtable.pickle\"), \"rb\") as f:\n",
    "            self.table = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nq2G-PMvTFyw"
   },
   "outputs": [],
   "source": [
    "# Табличная модель reinforcement learning\n",
    "class RLTDmodel:\n",
    "    def __init__(self, player, gameboard, qtable):\n",
    "        self.alpha = 0.1\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 0.1        \n",
    "        self.r = 0.\n",
    "        self.player = player\n",
    "        self.gameboard = gameboard\n",
    "        self.Qtable = qtable\n",
    "        self.prev_a = None\n",
    "        self.prev_s = None    \n",
    "    \n",
    "    def select_action(self):\n",
    "        # Выбор действия через epsilon-greedy стратегию\n",
    "        state = self.gameboard.get_state()\n",
    "        actions = self.player.get_actions(state)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return actions[np.random.randint(len(actions))]\n",
    "        else:                        \n",
    "            Qs = np.array([self.Qtable.get_Q(state, action) for action in actions])\n",
    "            return actions[np.argmax(Qs)]\n",
    "        \n",
    "    def act(self):\n",
    "        self.r = 0.\n",
    "        a = self.select_action()\n",
    "        self.prev_a = a\n",
    "        self.prev_s = self.gameboard.get_state().copy()        \n",
    "        self.player.make_move(tuple(a))        \n",
    "        \n",
    "    def get_reward(self):\n",
    "        self.r = gb.get_reward(self.player)\n",
    "\n",
    "    def update_Qs(self):\n",
    "        # Обновление Q по уравнению Беллмана\n",
    "        state = self.gameboard.get_state()\n",
    "        actions = self.player.get_actions(state)        \n",
    "        Q_old = self.Qtable.get_Q(self.prev_s, self.prev_a)\n",
    "        if not self.gameboard.done:\n",
    "            Q_max = np.max(np.array([self.Qtable.get_Q(state, action) for action in actions]))            \n",
    "        else:\n",
    "            Q_max = 0.            \n",
    "        Q_new = Q_old + self.alpha*(self.r + self.gamma*Q_max - Q_old)        \n",
    "        self.Qtable.set_Q(self.prev_s, self.prev_a, Q_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U0KsaQEsFhFO"
   },
   "outputs": [],
   "source": [
    "# Обучение модели.\n",
    "def train_qtable(num_iterations, start_epsilon, final_epsilon):\n",
    "    Xr = 0\n",
    "    Or = 0\n",
    "    Dr = 0\n",
    "    for i in range(num_iterations):\n",
    "        if i>1 and i % 10000 == 0:\n",
    "            qt.save()\n",
    "            print(\"Results {} :\".format(i), Xr, Dr, Or, str(100*Xr/(Xr+Or))+\"%\")\n",
    "            Xr = 0\n",
    "            Or = 0\n",
    "            Dr = 0\n",
    "        gb.reset()\n",
    "        # Уменьшение epsilon по мере обучения\n",
    "        rtm.epsilon = final_epsilon + (start_epsilon - final_epsilon)*((num_iterations - i)/num_iterations)\n",
    "        while not gb.done:\n",
    "            # Ход модели\n",
    "            rtm.act()            \n",
    "            if gb.done:\n",
    "                # Если терминальное состояние, получение награды (бот уже ходить не будет)\n",
    "                rtm.get_reward()\n",
    "                # Обновление таблицы\n",
    "                rtm.update_Qs()\n",
    "                if rtm.r == 1.:\n",
    "                    Xr += 1\n",
    "                if rtm.r == 0.1:\n",
    "                    Dr += 1\n",
    "            if(bot.act()):\n",
    "                # После хода бота, получение награды,\n",
    "                rtm.get_reward()            \n",
    "                # Обновление таблицы\n",
    "                rtm.update_Qs()\n",
    "                if gb.get_reward(bot) == 1.:\n",
    "                    Or += 1\n",
    "    print(\"Results\", Xr, Dr, Or, str(100*Xr/(Xr+Or))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "8KZOmpYb5IBp",
    "outputId": "9bc8907a-39d9-4f0b-af3c-28ea6b2ef2a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results 10000 : 1 7252 2747 0.036390101892285295%\n",
      "Results 20000 : 0 8002 1998 0.0%\n",
      "Results 30000 : 1 8171 1828 0.05467468562055768%\n",
      "Results 40000 : 6648 2018 1334 83.2873966424455%\n",
      "Results 50000 : 8310 592 1098 88.32908163265306%\n",
      "Results 60000 : 8557 520 923 90.26371308016877%\n",
      "Results 70000 : 8786 448 766 91.98073701842546%\n",
      "Results 80000 : 9062 318 620 93.59636438752324%\n",
      "Results 90000 : 9314 235 451 95.38146441372248%\n",
      "Results 9549 151 300 96.95400548279014%\n"
     ]
    }
   ],
   "source": [
    "# Обучение\n",
    "qt = QTable()\n",
    "gb = GameBoard()\n",
    "pla = Player(gb)\n",
    "bot = Player(gb, \"O\")\n",
    "rtm = RLTDmodel(pla, gb, qt)\n",
    "train_qtable(100000, 0.1, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "usk7GYMmTFzE"
   },
   "outputs": [],
   "source": [
    "# Обертка для игры для обучения нейросети. По ходу игрока возвращает следующее\n",
    "# состояние (после хода оппонента), награду, конец игры.\n",
    "class Game:\n",
    "    def __init__(self):\n",
    "        self.gb = GameBoard()\n",
    "        self.pla = Player(self.gb)\n",
    "        self.bot = Player(self.gb, \"O\")\n",
    "        self.actions_all = self.gb.get_actions()\n",
    "        self.actions_n = len(self.actions_all)\n",
    "    \n",
    "    def atoi(self, action):\n",
    "        return np.where(np.all(self.actions_all==action, axis=1))[0][0]\n",
    "    \n",
    "    def actions_valid_mask(self, state):\n",
    "        mask = np.zeros((1, 9))\n",
    "        v = self.pla.get_actions(state)\n",
    "        for v in v:\n",
    "            mask += np.all(self.actions_all==v, axis=1).astype(int)\n",
    "        return mask.astype(bool)         \n",
    "        \n",
    "    def step(self, action):\n",
    "        action = self.actions_all[np.array(action).astype(bool)][0]        \n",
    "        self.pla.make_move(tuple(action))\n",
    "        terminal = self.gb.done\n",
    "        # Получение награды либо в терминальном состоянии, либо после хода бота\n",
    "        if terminal:\n",
    "            reward = self.gb.get_reward(self.pla)        \n",
    "        else:\n",
    "            self.bot.act()\n",
    "            reward = self.gb.get_reward(self.pla)\n",
    "            terminal = self.gb.done\n",
    "        state = self.gb.get_state().copy()\n",
    "        return state, reward, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B6XvgpB_TFzM"
   },
   "outputs": [],
   "source": [
    "param_input_shape = 9\n",
    "param_n_actions = 9\n",
    "param_loss_fn = F.mse_loss\n",
    "param_gamma = 0.9\n",
    "param_epsilon = 0.1\n",
    "param_iterations = 100001\n",
    "param_replay_memory_size = 4096\n",
    "param_batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FnafWv6oTFzQ"
   },
   "outputs": [],
   "source": [
    "# FullyConnected с маской\n",
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, sizes, dropout=False, activation_fn=nn.Tanh(), flatten=False, \n",
    "                 last_fn=None, first_fn=None, device='cpu'):\n",
    "        super(FullyConnected, self).__init__()\n",
    "        layers = []\n",
    "        self.flatten = flatten\n",
    "        if first_fn is not None:\n",
    "            layers.append(first_fn)\n",
    "        for i in range(len(sizes) - 2):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
    "            if dropout:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            layers.append(activation_fn) # нам не нужен дропаут и фнкция активации в последнем слое\n",
    "        else: \n",
    "            layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
    "        if last_fn is not None:\n",
    "            layers.append(last_fn)\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x, mask=None):        \n",
    "        if self.flatten:\n",
    "            x = x.view(x.shape[0], -1)           \n",
    "        x = self.model(x)\n",
    "        # Маскирование невозможных действий нулями\n",
    "        if mask is not None:\n",
    "            x = x*mask.float()\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TIcmOR_OTFzU"
   },
   "outputs": [],
   "source": [
    "# Создание \"policy net\" и \"target net\"\n",
    "dqn = FullyConnected([param_input_shape, 128, 256, 128, param_n_actions],\n",
    "                     flatten=True, dropout=False, activation_fn=nn.LeakyReLU(0.2), last_fn=nn.Softmax(dim=-1), device=device)\n",
    "\n",
    "dqn_opt = optim.Adam(dqn.parameters(), lr=1e-5)\n",
    "\n",
    "dqn_target = FullyConnected([param_input_shape, 128, 256, 128, param_n_actions],\n",
    "                     flatten=True, dropout=False, activation_fn=nn.LeakyReLU(0.2), last_fn=nn.Softmax(dim=-1), device=device)\n",
    "dqn_target.load_state_dict(dqn.state_dict())\n",
    "dqn_target.eval()\n",
    "\n",
    "dqn_train_log = {'DQN': []}\n",
    "dqn_test_log = {'DQN': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lep2slGdDoCQ"
   },
   "outputs": [],
   "source": [
    "load_filename = \"model_128_256_128_softmax_start_v2_0_10000.pth\"\n",
    "filename = \"model_128_256_128_softmax_start_v2_1_{}.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ymo5nfjNddHq"
   },
   "outputs": [],
   "source": [
    "# Горячий старт\n",
    "dqn = FullyConnected([param_input_shape, 128, 256, 128, param_n_actions],\n",
    "                     flatten=True, dropout=False, activation_fn=nn.LeakyReLU(0.2), last_fn=nn.Softmax(dim=-1), device=device)\n",
    "\n",
    "dqn.load_state_dict(torch.load(os.path.join(PATH, load_filename)))\n",
    "dqn_opt = optim.Adam(dqn.parameters(), lr=1e-5)\n",
    "\n",
    "dqn_target = FullyConnected([param_input_shape, 128, 256, 128, param_n_actions],\n",
    "                     flatten=True, dropout=False, activation_fn=nn.LeakyReLU(0.2), last_fn=nn.Softmax(dim=-1), device=device)\n",
    "dqn_target.load_state_dict(dqn.state_dict())\n",
    "dqn_target.eval()\n",
    "\n",
    "dqn_train_log = {'DQN': []}\n",
    "dqn_test_log = {'DQN': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vXrIqWHITFzZ"
   },
   "outputs": [],
   "source": [
    "dqn_opt = optim.Adam(dqn.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vZw_tji6TFzd"
   },
   "outputs": [],
   "source": [
    "replay_memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "colab_type": "code",
    "id": "xDnp4sDyTFzh",
    "outputId": "6cb98800-652e-4027-fc6f-06ce5e55703e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000 loss: 0.010903745889663696, reward ratio: 0.082, wins: 1, draws: 167, losses 49\n",
      "Epoch 2000 loss: 0.010075964964926243, reward ratio: 0.007, wins: 0, draws: 23, losses 290\n",
      "Epoch 3000 loss: 0.00854954682290554, reward ratio: 0.009, wins: 1, draws: 18, losses 299\n",
      "Epoch 4000 loss: 0.005336948670446873, reward ratio: 0.077, wins: 0, draws: 167, losses 50\n",
      "Epoch 5000 loss: 0.004350479692220688, reward ratio: 0.009, wins: 1, draws: 20, losses 295\n",
      "Epoch 6000 loss: 0.004019974265247583, reward ratio: 0.049, wins: 1, draws: 114, losses 137\n",
      "Epoch 7000 loss: 0.0038592566270381212, reward ratio: 0.076, wins: 0, draws: 165, losses 53\n",
      "Epoch 8000 loss: 0.002630142727866769, reward ratio: 0.072, wins: 0, draws: 158, losses 62\n",
      "Epoch 9000 loss: 0.004744658246636391, reward ratio: 0.080, wins: 1, draws: 164, losses 52\n",
      "Epoch 10000 loss: 0.0058478666469454765, reward ratio: 0.081, wins: 2, draws: 158, losses 59\n",
      "Epoch 11000 loss: 0.0036895680241286755, reward ratio: 0.073, wins: 1, draws: 154, losses 69\n",
      "Epoch 12000 loss: 0.004969261586666107, reward ratio: 0.076, wins: 0, draws: 165, losses 51\n",
      "Epoch 13000 loss: 0.005232507362961769, reward ratio: 0.081, wins: 0, draws: 174, losses 40\n",
      "Epoch 14000 loss: 0.0058705173432827, reward ratio: 0.076, wins: 1, draws: 157, losses 61\n",
      "Epoch 15000 loss: 0.005817534402012825, reward ratio: 0.046, wins: 1, draws: 106, losses 147\n",
      "Epoch 16000 loss: 0.006341014988720417, reward ratio: 0.086, wins: 3, draws: 159, losses 57\n",
      "Epoch 17000 loss: 0.00529705174267292, reward ratio: 0.076, wins: 0, draws: 164, losses 53\n",
      "Epoch 18000 loss: 0.005099509377032518, reward ratio: 0.077, wins: 0, draws: 165, losses 50\n",
      "Epoch 19000 loss: 0.009001418016850948, reward ratio: 0.071, wins: 0, draws: 157, losses 63\n",
      "Epoch 20000 loss: 0.0051573412492871284, reward ratio: 0.081, wins: 1, draws: 165, losses 51\n",
      "Epoch 21000 loss: 0.008138597942888737, reward ratio: 0.015, wins: 0, draws: 44, losses 252\n",
      "Epoch 22000 loss: 0.008746899664402008, reward ratio: 0.072, wins: 0, draws: 158, losses 62\n",
      "Epoch 23000 loss: 0.006354619283229113, reward ratio: 0.075, wins: 0, draws: 162, losses 55\n",
      "Epoch 24000 loss: 0.007259588688611984, reward ratio: 0.078, wins: 1, draws: 160, losses 57\n"
     ]
    }
   ],
   "source": [
    "# Обучение\n",
    "for epoch in range(param_iterations):\n",
    "    g = Game()\n",
    "    # Прохождение игры против бота для заполнения replay memory\n",
    "    while not g.gb.done:\n",
    "        dqn.eval()\n",
    "        state_0 = torch.Tensor(g.gb.get_state().copy()).unsqueeze(0)\n",
    "        action = torch.zeros((1, param_n_actions))\n",
    "        if np.random.rand() < param_epsilon:\n",
    "            output = torch.rand((1,9))    \n",
    "        else:\n",
    "            output = dqn(state_0.to(device))\n",
    "        mask = torch.Tensor(g.actions_valid_mask(state_0.squeeze().numpy())).type(torch.uint8).to(device)\n",
    "        output[~mask] = 0\n",
    "        action[:, torch.argmax(output)] = 1\n",
    "        state_1, reward, terminal = g.step(action[0].numpy())\n",
    "        state_1 = torch.Tensor(state_1).unsqueeze(0).to(device)\n",
    "        reward = torch.Tensor([reward]).unsqueeze(0).to(device)\n",
    "        terminal = torch.Tensor([terminal]).type(torch.uint8).unsqueeze(0).to(device)\n",
    "\n",
    "        replay_memory.append((state_0, action, reward, state_1, terminal))\n",
    "        if len(replay_memory) > param_replay_memory_size:\n",
    "            replay_memory.pop(0)\n",
    "    \n",
    "    # Обучение сэмплированным батчем из replay memory\n",
    "    if len(replay_memory) > param_batch_size:\n",
    "        dqn.train()\n",
    "        dqn.zero_grad()\n",
    "        batch = random.sample(replay_memory, param_batch_size)\n",
    "    \n",
    "        # Разделение батча на признаки\n",
    "        batch_state_0 = torch.cat(tuple(d[0] for d in batch)).to(device)\n",
    "        batch_action = torch.cat(tuple(d[1] for d in batch)).to(device)\n",
    "        batch_reward = torch.cat(tuple(d[2] for d in batch)).to(device)\n",
    "        batch_state_1 = torch.cat(tuple(d[3] for d in batch)).to(device)\n",
    "        batch_terminal = torch.cat(tuple(d[4] for d in batch)).to(device)\n",
    "    \n",
    "        # Предсказание target net Q значений следующего состояниz\n",
    "        batch_mask_1 = (batch_state_1 == 0)\n",
    "        batch_mask_1 = batch_mask_1.view((param_batch_size, -1)).to(device)\n",
    "        output_1 = dqn_target(batch_state_1, batch_mask_1)        \n",
    "    \n",
    "        # Создание \"groun truth\" меток (r(s,a) + gamma*max(Q(s',a)))\n",
    "        batch_y = torch.cat(tuple(batch_reward[i] if batch_terminal[i] else batch_reward[i] + \\\n",
    "                                  param_gamma*torch.max(output_1[i]) for i in range(len(batch))))        \n",
    "\n",
    "        # Получение Q текущего состояния для функции потерь\n",
    "        batch_mask_0 = (batch_state_0 == 0)\n",
    "        batch_mask_0 = batch_mask_0.view((param_batch_size, -1)).to(device)\n",
    "        output_0 = dqn(batch_state_0, batch_mask_0) \n",
    "\n",
    "        # Значение Q, по которому было совершено действие\n",
    "        batch_q = torch.sum(output_0*batch_action, dim=1)        \n",
    "        \n",
    "        batch_y = batch_y.detach()\n",
    "        loss = param_loss_fn(batch_q, batch_y)        \n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        for param in dqn.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        dqn_opt.step()\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            # Обновление весов target net\n",
    "            dqn_target.load_state_dict(dqn.state_dict())\n",
    "            # Вывод результатов\n",
    "            res = torch.cat(tuple([d[2] for d in replay_memory[-1000:] if d[4]]))\n",
    "            res = torch.sum(res)/len(res)\n",
    "            wins = len([d[2] for d in replay_memory[-1000:] if d[2]==1])\n",
    "            draws = len([d[2] for d in replay_memory[-1000:] if d[2]==0.1])\n",
    "            losses = len([d[2] for d in replay_memory[-1000:] if (d[2]==0 and d[4])])\n",
    "            print(\"Epoch {} loss: {}, reward ratio: {:.3f}, wins: {}, draws: {}, losses {}\".format(epoch, loss.item(), res, wins, draws, losses))\n",
    "            if epoch % 10000 == 0:\n",
    "                torch.save(dqn.state_dict(), os.path.join(PATH, filename.format(epoch)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TicTacToe.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
