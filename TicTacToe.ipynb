{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "6p2ANr9STFyL",
    "outputId": "bec625fa-e49f-4aab-cc43-4c0ff940b0ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "colab_type": "code",
    "id": "Ts1CG_YjWDW6",
    "outputId": "095398b2-ca86-4654-88bd-f9e741c418e5"
   },
   "outputs": [],
   "source": [
    "PATH = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oZ8oaTqaTFyW"
   },
   "outputs": [],
   "source": [
    "# Игровое поле\n",
    "class GameBoard:\n",
    "    def __init__(self):        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.board = np.zeros((3, 3)).astype(int)\n",
    "        self.done = False\n",
    "        self.X_reward = 0.\n",
    "        self.O_reward = 0.\n",
    "    \n",
    "    def set_sign(self, coords, sign):\n",
    "        # Выполняет ход, если возможно\n",
    "        assert(type(coords) is tuple and len(coords)==2)\n",
    "        if self.board[coords] != 0:\n",
    "            return False\n",
    "        if sign==1 or sign==-1:\n",
    "            self.board[coords] = sign\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self.board\n",
    "    \n",
    "    def get_actions(self):\n",
    "        # Возвращает допустимые действия\n",
    "        return np.argwhere(self.board==0)\n",
    "    \n",
    "    def check_win(self, sign, state=None):\n",
    "        # Проверяет выигрыш игрока\n",
    "        if state is None:\n",
    "            state = self.board\n",
    "        win = False\n",
    "        for i in range(3):\n",
    "            if (state[i, :] == sign).sum() == 3:                \n",
    "                win = True\n",
    "            if (state[:, i] == sign).sum() == 3:\n",
    "                win = True\n",
    "        diag = np.array([state[0,0], state[1,1], state[2,2]])\n",
    "        if (diag == sign).sum() == 3:\n",
    "            win = True        \n",
    "        diag = np.array([state[2,0], state[1,1], state[0,2]])\n",
    "        if (diag == sign).sum() == 3:\n",
    "            win = True\n",
    "        return win\n",
    "    \n",
    "    def check_draw(self):\n",
    "        # Проверяет ничью\n",
    "        if 0 not in self.board:\n",
    "            return True\n",
    "    \n",
    "    def check_done(self):\n",
    "        # Проверяет завершение партии\n",
    "        if self.check_win(1):\n",
    "            #print(\"X won\")\n",
    "            self.X_reward = 1.\n",
    "            self.O_reward = 0.\n",
    "            self.done = True\n",
    "        if self.check_win(-1):\n",
    "            #print(\"O won\")\n",
    "            self.O_reward = 1.\n",
    "            self.X_reward = 0.\n",
    "            self.done = True\n",
    "        if self.check_draw():\n",
    "            #print(\"Draw\")\n",
    "            self.X_reward = 0.1\n",
    "            self.O_reward = 0.1\n",
    "            self.done = True        \n",
    "        return self.done\n",
    "    \n",
    "    def get_reward(self, player):\n",
    "        # Возвращает награду\n",
    "        if player.sign == 1:\n",
    "            return self.X_reward\n",
    "        if player.sign == -1:\n",
    "            return self.O_reward\n",
    "    \n",
    "    def print(self):\n",
    "        # Печатает поле\n",
    "        sign = lambda x: \"X\" if x == 1 else \"O\" if x == -1 else \" \"\n",
    "        print(\"  0 1 2 \")\n",
    "        print(\"  - - - \")\n",
    "        print(\"0|{}|{}|{}|\".format(sign(self.board[0,0]), sign(self.board[0,1]), sign(self.board[0,2])))\n",
    "        print(\"  - - - \")\n",
    "        print(\"1|{}|{}|{}|\".format(sign(self.board[1,0]), sign(self.board[1,1]), sign(self.board[1,2])))\n",
    "        print(\"  - - - \")\n",
    "        print(\"2|{}|{}|{}|\".format(sign(self.board[2,0]), sign(self.board[2,1]), sign(self.board[2,2])))\n",
    "        print(\"  - - - \")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q-rkVpsuTFyd"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Класс игрока.\n",
    "Включает бот для игры, стремящийся поставить три в ряд и\n",
    "блокирующий такие попытки оппонента. Пытается вилки делать.\n",
    "Простенький, но играть может.\n",
    "\"\"\"\n",
    "class Player:\n",
    "    def __init__(self, board, side=\"X\"):\n",
    "        self.board = board\n",
    "        self.sign = 1 if side==\"X\" else -1 if side==\"O\" else 0\n",
    "            \n",
    "    def __check_danger(self, coords, sign, state):  \n",
    "        # Проверяет опасную ситуацию      \n",
    "        c0 = coords[0]\n",
    "        c1 = coords[1]        \n",
    "        if (state[c0, :]==-sign).sum()==2:\n",
    "            return True\n",
    "        if (state[:, c1]==-sign).sum()==2:\n",
    "            return True\n",
    "        if c0==c1:\n",
    "            diag = np.array([state[0,0], state[1,1], state[2,2]])\n",
    "            if (diag==-sign).sum()==2:\n",
    "                return True\n",
    "        if np.abs(c0-c1)==2:\n",
    "            diag = np.array([state[2,0], state[1,1], state[0,2]])\n",
    "            if (diag==-sign).sum()==2:\n",
    "                return True\n",
    "        if c0==1 and c1==1:\n",
    "            diag = np.array([state[0,0], state[1,1], state[2,2]])\n",
    "            if (diag==-sign).sum()==2:\n",
    "                return True        \n",
    "            diag = np.array([state[2,0], state[1,1], state[0,2]])\n",
    "            if (diag==-sign).sum()==2:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def __find_danger(self, sign, state):         \n",
    "        return np.argwhere(state==0)[np.array([self.__check_danger(c, sign, state) for c in self.get_actions(state)])]\n",
    "    \n",
    "    def get_actions(self, state):\n",
    "        return np.argwhere(state==0)\n",
    "    \n",
    "    def make_move(self, coords):\n",
    "        # Для совершения хода вручную\n",
    "        if self.board.check_done():\n",
    "            return False\n",
    "        if self.board.set_sign(coords, self.sign):\n",
    "            self.board.check_done()\n",
    "            return True\n",
    "        print(\"Invalid move\")\n",
    "        return False\n",
    "    \n",
    "    def act(self):\n",
    "        # Совершение хода ботом\n",
    "        if self.board.check_done():\n",
    "            return False\n",
    "        wins = self.__find_danger(-self.sign, self.board.get_state())\n",
    "        if len(wins) > 0:\n",
    "            self.make_move(tuple(wins[0]))\n",
    "            return True\n",
    "        dangers = self.__find_danger(self.sign, self.board.get_state())\n",
    "        if len(dangers) > 0:\n",
    "            self.make_move(tuple(dangers[0]))\n",
    "            return True\n",
    "        actions = self.get_actions(self.board.get_state())\n",
    "        if [1, 1] in actions.tolist():            \n",
    "            self.make_move((1, 1))\n",
    "            return True\n",
    "        else:\n",
    "            potential = 0\n",
    "            action = actions[np.random.randint(len(actions))]\n",
    "            for a in actions:                \n",
    "                test_state = self.board.get_state().copy()\n",
    "                test_state[tuple(a)] = self.sign\n",
    "                if len(self.__find_danger(sign=-self.sign, state=test_state)) > potential:\n",
    "                    potential = len(self.__find_danger(sign=-self.sign, state=test_state))\n",
    "                    action = a                    \n",
    "            self.make_move(tuple(action))        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "lMAF3hXtTFyi",
    "outputId": "ccbe1498-bc22-4fbd-ee07-910ccdb70cdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 1 2 \n",
      "  - - - \n",
      "0| | | |\n",
      "  - - - \n",
      "1| | | |\n",
      "  - - - \n",
      "2| | | |\n",
      "  - - - \n"
     ]
    }
   ],
   "source": [
    "gb = GameBoard()\n",
    "pla = Player(gb)\n",
    "bot = Player(gb, \"O\")\n",
    "gb.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "UxRuHEIiTFyn",
    "outputId": "755b262d-b92f-46ea-ef6e-72366feb8bf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 1 2 \n",
      "  - - - \n",
      "0|O|O|X|\n",
      "  - - - \n",
      "1| |X|X|\n",
      "  - - - \n",
      "2|O| |X|\n",
      "  - - - \n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Можно поиграть вручную, задавая коордтнаты в make_move()\n",
    "if(pla.make_move((1,2))):\n",
    "    gb.print()\n",
    "    print(gb.get_reward(pla))\n",
    "    if(bot.act()):\n",
    "        gb.print()\n",
    "        print(gb.get_reward(bot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0nTxM9eETFyt"
   },
   "outputs": [],
   "source": [
    "# Класс таблицы Q(s,a)\n",
    "class QTable:\n",
    "    def __init__(self):\n",
    "        self.__generate_positions()\n",
    "        self.all_actions = np.array([p for p in itertools.product(range(3), repeat=2)])\n",
    "        self.__generate_table()\n",
    "    \n",
    "    def __check_position(self, pos):\n",
    "        # Проверяет валидность сгенерированных позиций\n",
    "        valid = True\n",
    "        count = 0           \n",
    "        for i in range(3):\n",
    "            if (pos[i, :] == 1).sum() == 3:\n",
    "                count += 1\n",
    "            if (pos[i, :] == -1).sum() == 3:\n",
    "                count += 1\n",
    "            if (pos[:, i] == 1).sum() == 3:\n",
    "                count += 1\n",
    "            if (pos[:, i] == -1).sum() == 3:\n",
    "                count += 1\n",
    "        diag = np.array([pos[0,0], pos[1,1], pos[2,2]])\n",
    "        if (diag == 1).sum() == 3:\n",
    "            count += 1\n",
    "        if (diag == -1).sum() == 3:\n",
    "            count += 1\n",
    "        diag = np.array([pos[2,0], pos[1,1], pos[0,2]])\n",
    "        if (diag == 1).sum() == 3:\n",
    "            count += 1\n",
    "        if (diag == -1).sum() == 3:\n",
    "            count += 1\n",
    "        if count >= 1:\n",
    "            valid = False\n",
    "        return valid\n",
    "    \n",
    "    def __generate_positions(self):\n",
    "        # Создает список всех возможных позиций.\n",
    "        positions = []\n",
    "        for i, c in enumerate(itertools.product(range(3), repeat=9)):\n",
    "            c = np.array(c)-1\n",
    "            if (c==1).sum() >= (c==-1).sum() and (c==1).sum() <= (c==-1).sum()+1:\n",
    "                c = c.reshape([3,3])\n",
    "                if self.__check_position(c):\n",
    "                    positions.append(c)\n",
    "        self.positions = np.array(positions).astype(int)\n",
    "    \n",
    "    def __generate_table(self):\n",
    "        # Создает q-таблицу с нулями и наградами на выигрышных позициях (для крестиков)\n",
    "        self.table = np.zeros((9, len(self.positions)))\n",
    "        for i, pos in enumerate(self.positions):\n",
    "            if GameBoard().check_win(1, pos):\n",
    "                self.table[:, i] = 1\n",
    "    \n",
    "    def patois(self, plaa):\n",
    "        # Возвращает индексы доступных игроку действий\n",
    "        return np.array([np.where(np.all(self.all_actions==plaa[i], axis=1))[0][0] for i in range(len(plaa))])\n",
    "    \n",
    "    def atoi(self, action):\n",
    "        # Возвращает индекс действия\n",
    "        return np.where(np.all(self.all_actions==action, axis=1))[0][0]\n",
    "    \n",
    "    def stoi(self, state):\n",
    "        # Возвращает индекс состояния\n",
    "        return np.where(np.all(self.positions == state, axis=(1,2)))[0][0]\n",
    "    \n",
    "    def get_states(self):\n",
    "        # Возвращает все позиции\n",
    "        return self.positions\n",
    "    \n",
    "    def get_table(self):\n",
    "        # Возвращает таблицу\n",
    "        return self.table\n",
    "    \n",
    "    def get_all_actions(self):\n",
    "        # Возвращает все действия\n",
    "        return self.all_actions\n",
    "    \n",
    "    def get_Q(self, state, action):\n",
    "        # Возвращает Q(s,a)\n",
    "        si = self.stoi(state)\n",
    "        ai = self.atoi(action)\n",
    "        return self.table[ai, si]\n",
    "    \n",
    "    def set_Q(self, state, action, value):\n",
    "        # Устанавливает Q(s,a)\n",
    "        si = self.stoi(state)\n",
    "        ai = self.atoi(action)        \n",
    "        self.table[ai, si] = value\n",
    "        \n",
    "    def get_Qs(self, state):\n",
    "        # Возвращает значения Q для всех действий при состоянии\n",
    "        si = self.stoi(state)        \n",
    "        return self.table[:, si]\n",
    "    \n",
    "    def save(self):\n",
    "        with open(\"qtable.pickle\", \"wb\") as f:\n",
    "            pickle.dump(self.table, f)\n",
    "            \n",
    "    def load(self):\n",
    "        with open(\"qtable.pickle\", \"rb\") as f:\n",
    "            self.table = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nq2G-PMvTFyw"
   },
   "outputs": [],
   "source": [
    "# Табличная модель reinforcement learning\n",
    "class RLTDmodel:\n",
    "    def __init__(self, player, gameboard, qtable):\n",
    "        self.alpha = 0.1\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 0.9\n",
    "        self.max_acts = 5\n",
    "        self.act_n = 1\n",
    "        self.r = 0.\n",
    "        self.player = player\n",
    "        self.gameboard = gameboard\n",
    "        self.Qtable = qtable\n",
    "        self.prev_a = None\n",
    "        self.prev_s = None    \n",
    "    \n",
    "    def select_action(self):\n",
    "        # Выбор действия через epsilon-greedy стратегию\n",
    "        actions = self.player.get_actions(self.gameboard.get_state())\n",
    "        if np.random.rand() < self.epsilon/self.act_n:            \n",
    "            return actions[np.random.randint(len(actions))]\n",
    "        else:            \n",
    "            state = self.gameboard.get_state()            \n",
    "            Qs = np.array([self.Qtable.get_Q(state, action) for action in actions])\n",
    "            return actions[np.argmax(Qs)]\n",
    "        \n",
    "    def act(self):\n",
    "        a = self.select_action()\n",
    "        self.prev_a = a\n",
    "        self.prev_s = self.gameboard.get_state().copy()        \n",
    "        self.player.make_move(tuple(a))\n",
    "        self.r = gb.get_reward(self.player)        \n",
    "        self.act_n += 1\n",
    "        \n",
    "    def update_Qs(self):\n",
    "        # Обновление Q по уравнению Беллмана\n",
    "        actions = self.player.get_actions(self.gameboard.get_state())\n",
    "        state = self.gameboard.get_state()        \n",
    "        Q_old = self.Qtable.get_Q(self.prev_s, self.prev_a)\n",
    "        if not self.gameboard.done:\n",
    "            Q_max = np.max(np.array([self.Qtable.get_Q(state, action) for action in actions]))            \n",
    "        else:\n",
    "            Q_max = 0.\n",
    "            self.act_n = 1        \n",
    "        Q_new = Q_old + self.alpha*(self.r + self.gamma*Q_max - Q_old)        \n",
    "        self.Qtable.set_Q(self.prev_s, self.prev_a, Q_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a7MTSeTXTFy1"
   },
   "outputs": [],
   "source": [
    "# Создание и загрузка обученной q таблицы\n",
    "qt = QTable()\n",
    "qt.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bZnpEBPnTFy5",
    "outputId": "5c060521-b616-4196-e5e2-978f12c326da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 1 2 \n",
      "  - - - \n",
      "0| | | |\n",
      "  - - - \n",
      "1| | | |\n",
      "  - - - \n",
      "2| | | |\n",
      "  - - - \n"
     ]
    }
   ],
   "source": [
    "gb = GameBoard()\n",
    "pla = Player(gb)\n",
    "bot = Player(gb, \"O\")\n",
    "# Создание модели\n",
    "rtm = RLTDmodel(pla, gb, qt)\n",
    "gb.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bfq0xYifTFy-",
    "outputId": "329053c9-5f52-44d7-8c20-d7a40bbd888d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results 655.0 126.0 83.86683738796415%\n"
     ]
    }
   ],
   "source": [
    "# Обучение модели. Постепенно начинает играть со 100% эффективностью, найдя все \n",
    "# уязвимости бота (если сделать epsilon маленьким)\n",
    "Xr = 0\n",
    "Or = 0\n",
    "rtm.epsilon = 0.3\n",
    "for i in range(1000):\n",
    "    if i>1 and i % 1000 == 0:\n",
    "        qt.save()\n",
    "        print(\"Results {} :\".format(i), Xr, Or, str(100*Xr/(Xr+Or))+\"%\")\n",
    "    gb.reset()\n",
    "    while not gb.done:\n",
    "        rtm.act()\n",
    "        #gb.print()\n",
    "        if gb.get_reward(pla) == 1.:\n",
    "            Xr += gb.get_reward(pla)\n",
    "        if gb.done:\n",
    "            rtm.update_Qs()\n",
    "        if(bot.act()):\n",
    "            #gb.print()\n",
    "            if gb.get_reward(bot) == 1.:\n",
    "                Or += gb.get_reward(bot)\n",
    "            rtm.update_Qs()\n",
    "print(\"Results\", Xr, Or, str(100*Xr/(Xr+Or))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "usk7GYMmTFzE"
   },
   "outputs": [],
   "source": [
    "# Обертка для игры для обучения нейросети. По ходу игрока возвращает следующее\n",
    "# состояние (после хода оппонента), награду, конец игры.\n",
    "class Game:\n",
    "    def __init__(self):\n",
    "        self.gb = GameBoard()\n",
    "        self.pla = Player(self.gb)\n",
    "        self.bot = Player(self.gb, \"O\")\n",
    "        self.actions_all = self.gb.get_actions()\n",
    "        self.actions_n = len(self.actions_all)\n",
    "    \n",
    "    def atoi(self, action):\n",
    "        return np.where(np.all(self.actions_all==action, axis=1))[0][0]\n",
    "    \n",
    "    def actions_valid_mask(self, state):\n",
    "        mask = np.zeros((1, 9))\n",
    "        v = self.pla.get_actions(state)\n",
    "        for v in v:\n",
    "            mask += np.all(self.actions_all==v, axis=1).astype(int)\n",
    "        return mask.astype(bool)         \n",
    "        \n",
    "    def step(self, action):\n",
    "        action = self.actions_all[np.array(action).astype(bool)][0]        \n",
    "        self.pla.make_move(tuple(action))\n",
    "        reward = self.gb.get_reward(self.pla)\n",
    "        terminal = self.gb.done\n",
    "        if not terminal:\n",
    "            self.bot.act()\n",
    "            reward = self.gb.get_reward(self.pla)\n",
    "            terminal = self.gb.done\n",
    "        state = self.gb.get_state().copy()\n",
    "        return state, reward, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B6XvgpB_TFzM"
   },
   "outputs": [],
   "source": [
    "param_input_shape = 9\n",
    "param_n_actions = 9\n",
    "param_loss_fn = F.mse_loss\n",
    "param_gamma = 0.9\n",
    "param_epsilon = 0.01\n",
    "param_iterations = 10001\n",
    "param_replay_memory_size = 4096\n",
    "param_batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FnafWv6oTFzQ"
   },
   "outputs": [],
   "source": [
    "# FullyConnected с маской\n",
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, sizes, dropout=False, activation_fn=nn.Tanh(), flatten=False, \n",
    "                 last_fn=None, first_fn=None, device='cpu'):\n",
    "        super(FullyConnected, self).__init__()\n",
    "        layers = []\n",
    "        self.flatten = flatten\n",
    "        if first_fn is not None:\n",
    "            layers.append(first_fn)\n",
    "        for i in range(len(sizes) - 2):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
    "            if dropout:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            layers.append(activation_fn) # нам не нужен дропаут и фнкция активации в последнем слое\n",
    "        else: \n",
    "            layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
    "        if last_fn is not None:\n",
    "            layers.append(last_fn)\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x, mask=None):        \n",
    "        if self.flatten:\n",
    "            x = x.view(x.shape[0], -1)           \n",
    "        x = self.model(x)\n",
    "        # Маскирование невозможных действий нулями\n",
    "        if mask is not None:\n",
    "            x = x*mask.float()\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TIcmOR_OTFzU"
   },
   "outputs": [],
   "source": [
    "# Создание \"policy net\" и \"target net\"\n",
    "dqn = FullyConnected([param_input_shape, 128, 256, 128, param_n_actions],\n",
    "                     flatten=True, dropout=0.3, activation_fn=nn.LeakyReLU(0.2), last_fn=nn.Softmax(dim=-1), device=device)\n",
    "\n",
    "dqn_opt = optim.Adam(dqn.parameters(), lr=1e-5)\n",
    "\n",
    "dqn_target = FullyConnected([param_input_shape, 128, 256, 128, param_n_actions],\n",
    "                     flatten=True, dropout=0.3, activation_fn=nn.LeakyReLU(0.2), last_fn=nn.Softmax(dim=-1), device=device)\n",
    "dqn_target.load_state_dict(dqn.state_dict())\n",
    "dqn_target.eval()\n",
    "\n",
    "dqn_train_log = {'DQN': []}\n",
    "dqn_test_log = {'DQN': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lep2slGdDoCQ"
   },
   "outputs": [],
   "source": [
    "load_filename = \"model_128_256_128_softmax_start_6_30000.pth\"\n",
    "filename = \"model_128_256_128_softmax_start_7_{}.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ymo5nfjNddHq"
   },
   "outputs": [],
   "source": [
    "# Горячий старт\n",
    "dqn = FullyConnected([param_input_shape, 128, 256, 128, param_n_actions],\n",
    "                     flatten=True, dropout=0.3, activation_fn=nn.LeakyReLU(0.2), last_fn=nn.Softmax(dim=-1), device=device)\n",
    "\n",
    "dqn.load_state_dict(torch.load(os.path.join(PATH, load_filename)))\n",
    "dqn_opt = optim.Adam(dqn.parameters(), lr=1e-5)\n",
    "\n",
    "dqn_target = FullyConnected([param_input_shape, 128, 256, 128, param_n_actions],\n",
    "                     flatten=True, dropout=0.3, activation_fn=nn.LeakyReLU(0.2), last_fn=nn.Softmax(dim=-1), device=device)\n",
    "dqn_target.load_state_dict(dqn.state_dict())\n",
    "dqn_target.eval()\n",
    "\n",
    "dqn_train_log = {'DQN': []}\n",
    "dqn_test_log = {'DQN': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vXrIqWHITFzZ"
   },
   "outputs": [],
   "source": [
    "dqn_opt = optim.Adam(dqn.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vZw_tji6TFzd"
   },
   "outputs": [],
   "source": [
    "replay_memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "xDnp4sDyTFzh",
    "outputId": "420ae572-11aa-47a8-8383-23f37f6671a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000 loss: 0.0017406800761818886, reward ratio: 0.961, wins: 241, draws: 2, losses 8\n",
      "Epoch 2000 loss: 0.002646678127348423, reward ratio: 0.974, wins: 243, draws: 4, losses 3\n",
      "Epoch 3000 loss: 0.0032253579702228308, reward ratio: 0.969, wins: 243, draws: 3, losses 5\n",
      "Epoch 4000 loss: 0.0014677841681987047, reward ratio: 0.965, wins: 241, draws: 2, losses 7\n",
      "Epoch 5000 loss: 0.003262968733906746, reward ratio: 0.992, wins: 248, draws: 1, losses 1\n",
      "Epoch 6000 loss: 0.0026034191250801086, reward ratio: 0.992, wins: 249, draws: 0, losses 2\n",
      "Epoch 7000 loss: 0.004114625044167042, reward ratio: 0.954, wins: 239, draws: 5, losses 7\n",
      "Epoch 8000 loss: 0.0029856807086616755, reward ratio: 0.961, wins: 242, draws: 1, losses 9\n",
      "Epoch 9000 loss: 0.0031430646777153015, reward ratio: 0.973, wins: 244, draws: 1, losses 6\n",
      "Epoch 10000 loss: 0.002826766576617956, reward ratio: 0.985, wins: 246, draws: 3, losses 1\n"
     ]
    }
   ],
   "source": [
    "# Обучение\n",
    "for epoch in range(param_iterations):\n",
    "    g = Game()\n",
    "    # Прохождение игры против бота для заполнения replay memory\n",
    "    while not g.gb.done:\n",
    "        dqn.eval()\n",
    "        state_0 = torch.Tensor(g.gb.get_state().copy()).unsqueeze(0)\n",
    "        action = torch.zeros((1, param_n_actions))\n",
    "        if np.random.rand() < param_epsilon:\n",
    "            output = torch.rand((1,9))    \n",
    "        else:\n",
    "            output = dqn(state_0.to(device))\n",
    "        mask = torch.Tensor(g.actions_valid_mask(state_0.squeeze().numpy())).type(torch.uint8).to(device)\n",
    "        output[~mask] = 0\n",
    "        action[:, torch.argmax(output)] = 1\n",
    "        state_1, reward, terminal = g.step(action[0].numpy())\n",
    "        state_1 = torch.Tensor(state_1).unsqueeze(0).to(device)\n",
    "        reward = torch.Tensor([reward]).unsqueeze(0).to(device)\n",
    "        terminal = torch.Tensor([terminal]).type(torch.uint8).unsqueeze(0).to(device)\n",
    "\n",
    "        replay_memory.append((state_0, action, reward, state_1, terminal))\n",
    "        if len(replay_memory) > param_replay_memory_size:\n",
    "            replay_memory.pop(0)\n",
    "    \n",
    "    # Обучение сэмплированным батчем из replay memory\n",
    "    if len(replay_memory) > param_batch_size:\n",
    "        dqn.train()\n",
    "        dqn.zero_grad()\n",
    "        batch = random.sample(replay_memory, param_batch_size)\n",
    "    \n",
    "        # Разделение батча на признаки\n",
    "        batch_state_0 = torch.cat(tuple(d[0] for d in batch)).to(device)\n",
    "        batch_action = torch.cat(tuple(d[1] for d in batch)).to(device)\n",
    "        batch_reward = torch.cat(tuple(d[2] for d in batch)).to(device)\n",
    "        batch_state_1 = torch.cat(tuple(d[3] for d in batch)).to(device)\n",
    "        batch_terminal = torch.cat(tuple(d[4] for d in batch)).to(device)\n",
    "    \n",
    "        # Предсказание target net Q значений следующего состояниz\n",
    "        batch_mask_1 = (batch_state_1 == 0)\n",
    "        batch_mask_1 = batch_mask_1.view((param_batch_size, -1)).to(device)\n",
    "        output_1 = dqn_target(batch_state_1, batch_mask_1)        \n",
    "    \n",
    "        # Создание \"groun truth\" меток (r(s,a) + gamma*max(Q(s',a)))\n",
    "        batch_y = torch.cat(tuple(batch_reward[i] if batch_terminal[i] else batch_reward[i] + \\\n",
    "                                  param_gamma*torch.max(output_1[i]) for i in range(len(batch))))        \n",
    "\n",
    "        # Получение Q текущего состояния для функции потерь\n",
    "        batch_mask_0 = (batch_state_0 == 0)\n",
    "        batch_mask_0 = batch_mask_0.view((param_batch_size, -1)).to(device)\n",
    "        output_0 = dqn(batch_state_0, batch_mask_0) \n",
    "\n",
    "        # Значение Q, по которому было совершено действие\n",
    "        batch_q = torch.sum(output_0*batch_action, dim=1)        \n",
    "        \n",
    "        batch_y = batch_y.detach()\n",
    "        loss = param_loss_fn(batch_q, batch_y)        \n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        for param in dqn.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        dqn_opt.step()\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            # Обновление весов target net\n",
    "            dqn_target.load_state_dict(dqn.state_dict())\n",
    "            # Вывод результатов\n",
    "            res = torch.cat(tuple([d[2] for d in replay_memory[-1000:] if d[4]]))\n",
    "            res = torch.sum(res)/len(res)\n",
    "            wins = len([d[2] for d in replay_memory[-1000:] if d[2]==1])\n",
    "            draws = len([d[2] for d in replay_memory[-1000:] if d[2]==0.1])\n",
    "            losses = len([d[2] for d in replay_memory[-1000:] if (d[2]==0 and d[4])])\n",
    "            print(\"Epoch {} loss: {}, reward ratio: {:.3f}, wins: {}, draws: {}, losses {}\".format(epoch, loss.item(), res, wins, draws, losses))\n",
    "            if epoch % 10000 == 0:\n",
    "                torch.save(dqn.state_dict(), os.path.join(PATH, filename.format(epoch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H4JYIKQ-qjac"
   },
   "source": [
    "Сеть тоже научилась играть против бота, хотя здесь результаты выглядят менее стабильными. Потребовалось около 300000 батчей (по инерции назвал переменную эпохи) по 1024 наблюдения из replay memory. Эпсилон снижался от 0.3 до 0.01."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TicTacToe-001.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
